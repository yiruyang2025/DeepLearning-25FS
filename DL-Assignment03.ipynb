{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KOKsRm40PW3"
      },
      "source": [
        "### Group Members:\n",
        "\n",
        "- Name, matriculation number\n",
        "- Name, matriculation number\n",
        "- Name, matriculation number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1B73Jzx85LV"
      },
      "source": [
        "# Assignment 3 - Convolutional Network and Transfer Learning\n",
        "\n",
        "The goal of this exercise is to learn how to train a small convolutional neural network (CNN) and fine-tune this trained network in transfer learning tasks.\n",
        "\n",
        "Our CNN has the following layers:\n",
        "\n",
        "1. 2D convolutional layer with $Q_1$ channels, kernel size $7\\times7$, stride 1 and padding 0.\n",
        "2. 2D maximum pooling with pooling size $2\\times2$ and stride 2\n",
        "3. `Tanh` activation\n",
        "4. 2D convolutional layer with $Q_2$ channels, kernel size $5\\times5$, stride 1 and padding 2.\n",
        "5. 2D maximum pooling with pooling size $2\\times2$ and stride 2\n",
        "6. `Tanh` activation\n",
        "7. A flattening layer to turn the 3D image into a 1D vector\n",
        "8. A fully-connected layer with the appropriate number of inputs and $O$ outputs.\n",
        "\n",
        "For this exercise, we will switch to an implementation in `PyTorch`.\n",
        "We will get used to some concepts in `PyTorch`, such as relying on the `torch.tensor` data structure, implementing the network, the loss functions, the training loop, and accuracy computation, which we will apply to categorical classification.\n",
        "We will see how backpropagation and weight update will be done automatically by `torch`.\n",
        "\n",
        "Please make sure that all your variables are compatible with `torch`.\n",
        "For example, you cannot mix `torch.tensor`s and `numpy.array`s in any part of the code.\n",
        "\n",
        "The CNN will be trained on the `letters` from EMNIST datasets and fine-tuned with the `digits` from the same dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dKg8Xm0Y8_OI"
      },
      "source": [
        "## Theoretical Sections\n",
        "\n",
        "In this section, we analyze the key properties of a CNN focusing on its spatial characteristics and parameterization. Specifically, we address the following fundamental aspects:  \n",
        "\n",
        "1. Receptive Field Computation  \n",
        "2. Feature Map Dimensions\n",
        "3. Learnable Parameter Count  \n",
        "4. Derivatives of Pooling\n",
        "\n",
        "Through these computations, we gain insights into the network's ability to capture spatial hierarchies and its overall complexity.\n",
        "\n",
        "Besides, we also point out some possible problems that implementing the training process may face."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJJfBzVDxTX2"
      },
      "source": [
        "#### Task 1.1: Compute Receptive Field\n",
        "\n",
        "Compute the receptive field size of an element of the final pooling layers before the flattening operation.\n",
        "\n",
        "Hints:\n",
        "\n",
        "- Consider one location in the last feature map, i.e., after the second pooling layer. Go backwards through the layers and compute the size of the receptive field for each operation. \n",
        "- Look at the attached graphic to understand how to compute receptive fields for convolution and pooling layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmGZrDkEz5Kr"
      },
      "source": [
        "Answer: ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBaG46cuevR-"
      },
      "source": [
        "#### Task 1.2: Compute Learnable Parameters\n",
        "\n",
        "Given that the input image has shape $28\\times28$,\n",
        "\n",
        "1. Compute feature map size, i.e., the number of inputs for the last fully connected layer.\n",
        "2. Estimate roughly how many learnable parameters this network has by analytically computing and adding the number of parameters in each layer. Express the estimation in terms of $Q_1, Q_2, O$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_T-ZwNwz8qj"
      },
      "source": [
        "Answer: ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IkrbNdWPREg"
      },
      "source": [
        "#### Task 1.3: Derivatives of Pooling\n",
        "\n",
        "Given two pooling methods:\n",
        "\n",
        "1. Average pooling: $$a_k=\\frac1R \\sum\\limits_{j=1}^R \\hat a_{{R(k-1)+j}}$$\n",
        "2. Maximum pooling: $$a_k=\\max\\limits_{j \\in \\{1,\\ldots, R\\}} \\hat a_{{R(k-1)+j}}$$\n",
        "\n",
        "Compute $$\\frac{\\partial a_k}{\\partial \\hat a_{R(k-1)+j}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iZdc_pb0AyU"
      },
      "source": [
        "Answer: ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFvSJ9eQPTTT"
      },
      "source": [
        "#### Task 1.4\n",
        "\n",
        "For a randomly initialized network, what is the expected loss value for binary classification and categorical classification?\n",
        "\n",
        "Make use of the loss functions provided below:\n",
        "\n",
        "$$\n",
        "\\mathcal{J}^{\\text{BCE}} = - \\frac{1}{N} \\sum_{n=1}^{N} \\left[ t^{[n]} \\log(y^{[n]}) + (1 - t^{[n]}) \\log(1 - y^{[n]}) \\right]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathcal{J}^{\\text{CCE}} = - \\sum_{n=1}^{N} \\sum_{o=1}^{O} t_o^{[n]} \\log y_o^{[n]}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gsCkMbq0Ely"
      },
      "source": [
        "Answer: ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruDkUgSPUN8"
      },
      "source": [
        "#### Task 1.5:\n",
        "\n",
        "Given the example loss and accuracy plots, compared with the standard plots, in which the learning rate is 0.001 and the computation for loss and accuracy is correct, analyze the possible problems for the other plots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGwOhdlM0HYn"
      },
      "source": [
        "Answer: ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ke4nefO9HHO"
      },
      "source": [
        "## Coding\n",
        "\n",
        "\n",
        "**<font color='red' size='5'>This section has to be submitted by 11:59 p.m. on Wednesday, April 9, to be graded.</font>**\n",
        "\n",
        "Before we start, we should assure that we have activated CUDA -- otherwise training might take very long.\n",
        "In Google Colaboratory:\n",
        "\n",
        "1. Check the options Runtime -> Change Runtime Type on top of the page.\n",
        "2. In the popup window, select hardware accelerator GPU.\n",
        "\n",
        "Afterward, the following command should run successfully:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPstyY7AaQLv",
        "outputId": "88ff8579-5b3d-4737-89cf-dfba715a9de2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA processing not available. Things will be slow :-(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"Successfully enabled CUDA processing\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"CUDA processing not available. Things will be slow :-(\")\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRoGfzzO9IMS"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "In PyTorch, a dataset stores a list of input and target tensors $(X^{[n]}, T^{[n]})$.\n",
        "We will make use of EMNIST dataset for this assignment.\n",
        "In particular, we will use the letters to train the CNN for classification task and then fine-tune this CNN using the digits.\n",
        "\n",
        "In the **EMNIST** dataset, the inputs are $X^{[n]} \\in \\mathbb R^{28\\times28}$. \n",
        "In case of the `letters` split, the labels are $T^{[n]} \\in \\{1,\\ldots,26\\}$.\n",
        "For `digits` split, the labels $T^{[n]} \\in \\{0,\\ldots,9\\}$ correspond to the digit.\n",
        "\n",
        "More precisely, the data in the dataset is provided in the form of `PIL.Image.Image`, which represents an image class with some more functionality, and pixel values in range $[0, 255]$.\n",
        "To convert these images into `torch.Tensor`'s in range $[0,1]$, we can use the `torchvision.transforms.ToTensor` transform.\n",
        "Furthermore, in `PyTorch` batches are created from datasets using the `torch.utils.data.DataLoader` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiM8WA30-C9q"
      },
      "source": [
        "#### Task 2.1: Dataset Loading\n",
        "\n",
        "Here, we use the letters (`split=\"letters\"`) in EMNIST dataset of gray images for categorical classification, and digits (`split=\"digits\"`) for transfer learning.\n",
        "\n",
        "Write a function that returns the training and the validation set of the dataset, using the given `transform` and `split`.\n",
        "\n",
        "Note:\n",
        "\n",
        "Targets for `letters` range $[1,26]$ by default, which will cause problem when using the loss desired function (which accepts $[0,25]$ instead) in `PyTorch`.\n",
        "Set `target_transform` to a function that can shift the target by subtracting 1 when `split=\"letters\".`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "uOXcGrpq_Wyb"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "\n",
        "def datasets(split, transform):\n",
        "    # 当使用 \"letters\" split 时，标签范围默认为 [1,26]，\n",
        "    # 但损失函数要求标签范围为 [0,25]，因此定义 target_transform 进行调整\n",
        "    if split == \"letters\":\n",
        "        target_transform = lambda x: x - 1\n",
        "    else:\n",
        "        target_transform = None\n",
        "\n",
        "    trainset = torchvision.datasets.EMNIST(\n",
        "        root='./data', \n",
        "        split=split,\n",
        "        train=True, \n",
        "        download=True, \n",
        "        transform=transform,\n",
        "        target_transform=target_transform\n",
        "    )\n",
        "    \n",
        "    validset = torchvision.datasets.EMNIST(\n",
        "        root='./data', \n",
        "        split=split,\n",
        "        train=False, \n",
        "        download=True, \n",
        "        transform=transform,\n",
        "        target_transform=target_transform\n",
        "    )\n",
        "    \n",
        "    return trainset, validset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Abp751vaQLx"
      },
      "source": [
        "#### Test 1: Data Types\n",
        "\n",
        "Create the dataset with `transform=None`.\n",
        "Check that all inputs are of type `PIL.Image.Image`, and all targets are integral."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAMpr6hhaQLx",
        "outputId": "58c5e70b-40a2-4e64-95e0-05e398bcba91"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 31%|███       | 172M/562M [1:24:51<3:12:40, 33.7kB/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m splits = [\u001b[33m\"\u001b[39m\u001b[33mletters\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdigits\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m splits:\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     trainset, validset = \u001b[43mdatasets\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m x,t \u001b[38;5;129;01min\u001b[39;00m trainset:\n\u001b[32m      7\u001b[39m         \u001b[38;5;66;03m# check datatype of input x\u001b[39;00m\n\u001b[32m      8\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, PIL.Image.Image)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mdatasets\u001b[39m\u001b[34m(split, transform)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m      9\u001b[39m     target_transform = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m trainset = \u001b[43mtorchvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEMNIST\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./data\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtarget_transform\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m validset = torchvision.datasets.EMNIST(\n\u001b[32m     21\u001b[39m     root=\u001b[33m'\u001b[39m\u001b[33m./data\u001b[39m\u001b[33m'\u001b[39m, \n\u001b[32m     22\u001b[39m     split=split,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     target_transform=target_transform\n\u001b[32m     27\u001b[39m )\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m trainset, validset\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/datasets/mnist.py:299\u001b[39m, in \u001b[36mEMNIST.__init__\u001b[39m\u001b[34m(self, root, split, **kwargs)\u001b[39m\n\u001b[32m    297\u001b[39m \u001b[38;5;28mself\u001b[39m.training_file = \u001b[38;5;28mself\u001b[39m._training_file(split)\n\u001b[32m    298\u001b[39m \u001b[38;5;28mself\u001b[39m.test_file = \u001b[38;5;28mself\u001b[39m._test_file(split)\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[38;5;28mself\u001b[39m.classes = \u001b[38;5;28mself\u001b[39m.classes_split_dict[\u001b[38;5;28mself\u001b[39m.split]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/datasets/mnist.py:100\u001b[39m, in \u001b[36mMNIST.__init__\u001b[39m\u001b[34m(self, root, train, transform, target_transform, download)\u001b[39m\n\u001b[32m     97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_exists():\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataset not found. You can use download=True to download it\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/datasets/mnist.py:336\u001b[39m, in \u001b[36mEMNIST.download\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    334\u001b[39m os.makedirs(\u001b[38;5;28mself\u001b[39m.raw_folder, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m336\u001b[39m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    337\u001b[39m gzip_folder = os.path.join(\u001b[38;5;28mself\u001b[39m.raw_folder, \u001b[33m\"\u001b[39m\u001b[33mgzip\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m gzip_file \u001b[38;5;129;01min\u001b[39;00m os.listdir(gzip_folder):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/datasets/utils.py:391\u001b[39m, in \u001b[36mdownload_and_extract_archive\u001b[39m\u001b[34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[32m    389\u001b[39m     filename = os.path.basename(url)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    393\u001b[39m archive = os.path.join(download_root, filename)\n\u001b[32m    394\u001b[39m extract_archive(archive, extract_root, remove_finished)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/datasets/utils.py:130\u001b[39m, in \u001b[36mdownload_url\u001b[39m\u001b[34m(url, root, filename, md5, max_redirect_hops)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# download the file\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[43m_urlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib.error.URLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    132\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m url[:\u001b[32m5\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mhttps\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/datasets/utils.py:30\u001b[39m, in \u001b[36m_urlretrieve\u001b[39m\u001b[34m(url, filename, chunk_size)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m urllib.request.urlopen(urllib.request.Request(url, headers={\u001b[33m\"\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh, tqdm(total=response.length, unit=\u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m, unit_scale=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m         \u001b[38;5;28;01mwhile\u001b[39;00m chunk := \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     31\u001b[39m             fh.write(chunk)\n\u001b[32m     32\u001b[39m             pbar.update(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/http/client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/socket.py:708\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    706\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    707\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    710\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1252\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1250\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1251\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1252\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1254\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/ssl.py:1104\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1104\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1105\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1106\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import PIL\n",
        "splits = [\"letters\",\"digits\"]\n",
        "for split in splits:\n",
        "    trainset, validset = datasets(split=split,transform=None)\n",
        "\n",
        "    for x,t in trainset:\n",
        "        # check datatype of input x\n",
        "        assert isinstance(x, PIL.Image.Image)\n",
        "        # check datatype of target t\n",
        "        assert isinstance(t, int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLckIo8E0h6h"
      },
      "source": [
        "#### Task 2.2: Data Visulization\n",
        "Create the dataset with `transform=None`.\n",
        "Use `matplotlib` to make a plot with 4 rows and 10 columns.\n",
        "\n",
        "Since all images in EMNIST dataset have been encoded with mixed-up horizontal and vertical axes, we want to plot the original image, as well as the version with a fixed orientation.\n",
        "\n",
        "Specifically, in the first row plot 10 images of letter trainset, and in the second row, plot them again with correct orientation. In the third row plot 10 images of digit trainset, and in the fourth row, plot them again with correct orientation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5KVLAAmM1D1"
      },
      "outputs": [],
      "source": [
        "# Load dataset without transformation (to show original)\n",
        "train_letters, _ = datasets(split=\"letters\",transform=None)\n",
        "train_digits, _ = datasets(split=\"digits\",transform=None)\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "\n",
        "pyplot.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# Function to fix exchanged horizontal and vertical axes\n",
        "def fix_orientation(img):\n",
        "    return np.array(img).T\n",
        "\n",
        "# Visualization\n",
        "fig, axes = pyplot.subplots(...)\n",
        "\n",
        "# Select 10 samples\n",
        "index = 0\n",
        "for j in range(10):\n",
        "    # Load letter\n",
        "    img_letter, _ = ...\n",
        "    fixed_letter = ...\n",
        "\n",
        "    # Load digit\n",
        "    img_digit, _ = ...\n",
        "    fixed_digit = ...\n",
        "\n",
        "    # Plot raw and fixed letters\n",
        "    axes[0, j].imshow(...)\n",
        "    axes[1, j].imshow(...)\n",
        "\n",
        "    # Plot raw and fixed digits\n",
        "    axes[2, j].imshow(...)\n",
        "    axes[3, j].imshow(...)\n",
        "\n",
        "    # Remove axes\n",
        "    for i in range(4):\n",
        "        axes[i, j].axis(\"off\")\n",
        "\n",
        "    index += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEhGkNEdaQLy"
      },
      "source": [
        "#### Task 2.3: Data Loaders\n",
        "\n",
        "Create two datasets by two splits with `transform=torchvision.transforms.ToTensor()`.\n",
        "For each dataset, create two data loaders, one for the training set and one for the validation set.\n",
        "The training batch size should be $B=64$, for the validation set, you can choose any batch size of your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A12taRZ6Dn76"
      },
      "outputs": [],
      "source": [
        "transform = ...\n",
        "trainset_letters, validset_letters = datasets(split=\"letters\",transform=transform)\n",
        "trainset_digits, validset_digits = datasets(split=\"digits\",transform=transform)\n",
        "\n",
        "B = 64\n",
        "\n",
        "trainloader_letters = torch.utils.data.DataLoader(...)\n",
        "validloader_letters = torch.utils.data.DataLoader(...)\n",
        "\n",
        "trainloader_digits = torch.utils.data.DataLoader(...)\n",
        "validloader_digits = torch.utils.data.DataLoader(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VmIeKXQaQLz"
      },
      "source": [
        "#### Test 2: Batches\n",
        "\n",
        "Check that all inputs and targets are of type `torch.Tensor`.\n",
        "\n",
        "Check that all input values are in range $[0,1]$.\n",
        "\n",
        "Check that all target values are in range $[0,25]$ for letters and $[0,9]$ for digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "XQ-wtDlPaQL0"
      },
      "outputs": [],
      "source": [
        "for x,t in trainloader_letters:\n",
        "    # check datatype, size and content of x\n",
        "    assert isinstance(x, torch.Tensor)\n",
        "    assert torch.all(x >= 0) and torch.all(x <= 1)\n",
        "    # check datatype, size and content of t\n",
        "    assert isinstance(t, torch.Tensor)\n",
        "    assert torch.all(t >= 0) and torch.all(t <= 25)\n",
        "\n",
        "\n",
        "for x,t in trainloader_digits:\n",
        "    # check datatype, size and content of x\n",
        "    assert isinstance(x, torch.Tensor)\n",
        "    assert torch.all(x >= 0) and torch.all(x <= 1)\n",
        "\n",
        "    # check datatype, size and content of t\n",
        "    assert isinstance(t, torch.Tensor)\n",
        "    assert torch.all(t >= 0) and torch.all(t <= 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCcVzXbiaQL0"
      },
      "source": [
        "### Convolutional Network Training\n",
        "\n",
        "For training and evaluating the network, we will rely on standard functionality in PyTorch.\n",
        "We will use the standard categorical cross-entropy loss together with a stochastic gradient descent optimizer.\n",
        "For training, we will use the batched implementation of the dataset, for which we perform one update step for each training batch.\n",
        "\n",
        "For each epoch, we will compute and save the average loss and accuracy for the full training set and validation set.\n",
        "This will be used to visualize the training process of CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIGWi22Fse7M"
      },
      "source": [
        "#### Task 2.4: Training and Validation Loop\n",
        "\n",
        "\n",
        "Implement a function that takes the network, train loader, validation loader, the number of epochs, the learning rate, and the momentum.\n",
        "Select the correct loss function for categorical classification and SGD optimizer.\n",
        "Iterate the following steps for the given number of epochs:\n",
        "\n",
        "1. Train the network with all batches of the training data.\n",
        "2. Compute the train set loss, train set accuracy, validation set loss, validation set accuracy.\n",
        "3. Store all in lists, for later visualization of CNN training process.\n",
        "\n",
        "Finally, return the lists of train losses and accuracies, as well as validation losses and accuracies.\n",
        "\n",
        "Notes:\n",
        "\n",
        "- Make sure that you train on the training data only, and `not` on the validation data.\n",
        "\n",
        "- When you iterate over validation set, please use `with torch.no_grad():` and loop on validloader inside it. This disables gradient computation and therefore saves memory.\n",
        "\n",
        "- While saving loss values, please use `.item()`.\n",
        "\n",
        "- Make sure that you divide the summed loss and accuracy values by the correct count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AThakWu18_Qb"
      },
      "outputs": [],
      "source": [
        "def training_loop(network, trainloader, validloader, epochs, lr, momentum):\n",
        "\n",
        "    network.to(device)\n",
        "\n",
        "    # select loss function and optimizer\n",
        "    loss_function = ...\n",
        "    optimizer = ...\n",
        "\n",
        "    # collect loss values and accuracies over the training epochs\n",
        "    train_loss_list, train_acc_list = [], []\n",
        "    val_loss_list, val_acc_list = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # train network on training data\n",
        "        train_loss, train_correct, train_total = 0, 0, 0\n",
        "        for x, t in trainloader:\n",
        "            # put data to device\n",
        "            ...\n",
        "            # train\n",
        "            ...\n",
        "            # weight update\n",
        "            ...\n",
        "            # calculate training accuracies and losses for current batch\n",
        "            ...\n",
        "        # append training accuracies and losses for current epoch\n",
        "        train_loss_list.append(...)\n",
        "        train_acc_list.append(...)\n",
        "\n",
        "        # validate network on validation data\n",
        "        val_loss, val_correct, val_total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for x, t in validloader:\n",
        "                # put data to device\n",
        "                ...\n",
        "                # compute validation loss\n",
        "                ...\n",
        "                # compute validation accuracy\n",
        "                ...\n",
        "\n",
        "        # append validation accuracies and losses for current epoch\n",
        "        val_loss_list.append(...)\n",
        "        val_acc_list.append(...)\n",
        "\n",
        "        # print training loss, accuracy, validation loss, accuracy for current epoch\n",
        "        print(...)\n",
        "\n",
        "\n",
        "    return train_loss_list, train_acc_list, val_loss_list, val_acc_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZIMk6NnqAou"
      },
      "source": [
        "#### Task 2.5: Convolutional Network\n",
        "\n",
        "We will rely on `torch.nn.Sequential` to create networks with particular lists of consecutive layers.\n",
        "\n",
        "Implement a function that generates a convolutional network with the following layers:\n",
        "\n",
        "1. 2D convolutional layer with $Q_1$ channels, kernel size $7\\times7$, stride 1 and padding 0.\n",
        "2. 2D maximum pooling with pooling size $2\\times2$ and stride 2\n",
        "3. `tanh` activation\n",
        "4. 2D convolutional layer with $Q_2$ channels, kernel size $5\\times5$, stride 1 and padding 2.\n",
        "5. 2D maximum pooling with pooling size $2\\times2$ and stride 2\n",
        "6. `tanh` activation\n",
        "7. A flattening layer to turn the 3D feature map into a 1D vector\n",
        "8. A fully-connected layer with the appropriate number of inputs and $O$ outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WhwVacT4Fwf_"
      },
      "outputs": [],
      "source": [
        "def convolutional(Q1, Q2, O):\n",
        "    return torch.nn.Sequential(\n",
        "        ...\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JbAXIU5qjmV"
      },
      "source": [
        "#### Test 3: Network Implementation\n",
        "\n",
        "Create a network with an arbitrary shape.\n",
        "\n",
        "Create a batch that follows input dimensions.\n",
        "\n",
        "Forward the batch through the network.\n",
        "\n",
        "Check that the output dimensions fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RsdB2be_qjmV"
      },
      "outputs": [],
      "source": [
        "net_ = convolutional(3,4,6)\n",
        "batch_ = torch.rand((8,1,28,28))\n",
        "output_ = net_(batch_)\n",
        "assert output_.shape == (8,6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzPxj34Os1Vw"
      },
      "source": [
        "#### Task 2.6: Convolutional Training\n",
        "\n",
        "Create a convolutional network with $Q_1=16$ and $Q_2=16$ convolutional channels and $O=26$ output neurons.\n",
        "Train the network for 5 epochs with $\\eta=0.01$, $\\text{momentum}=0.9$ and store the obtained train losses, accuracies, test losses and accuracies.\n",
        "\n",
        "If you want, you can also train for 20 epochs, the training time will increase accordingly -- it might take up to 30 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVkdyn2as1IL"
      },
      "outputs": [],
      "source": [
        "initial_network = convolutional(...)\n",
        "train_loss, train_acc, val_loss, val_acc = training_loop(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9Y5N1AKtDO8"
      },
      "source": [
        "#### Task 2.7: Plotting\n",
        "\n",
        "Plot the loss values in one plot and accuracy values into another plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uQoO7r8tLnH"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot\n",
        "pyplot.figure(figsize=(10,3))\n",
        "ax = pyplot.subplot(121)\n",
        "# plot training and validation loss values of CV network over epochs\n",
        "...\n",
        "\n",
        "ax = pyplot.subplot(122)\n",
        "# plot training and validation accuracy values of CV network over epochs\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBEhfACxteNE"
      },
      "source": [
        "### Transfer Learning\n",
        "\n",
        "We will make use of above trained CNN, which can be used to classify 26 handwritten characters, and fine-tune this CNN as to the digits classification task instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc2u98rmthUt"
      },
      "source": [
        "#### Task 2.8: Pre-trained Network Instantiation\n",
        "\n",
        "Make a copy of the trained CNN.\n",
        "\n",
        "Freeze the feature layers of the copied network.\n",
        "\n",
        "Notes:\n",
        "\n",
        "- To freeze layers, you can simply disable gradient computation for all learnable parameters of the network via `parameter.requires_grad = False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNVa6gDauhL2"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "# copy trained convolutional network\n",
        "network_copy = copy.deepcopy(initial_network)\n",
        "\n",
        "# Make sure to freeze all the layers of the network.\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1rv5U-mWLie"
      },
      "source": [
        "#### Task 2.9: Network Implementation\n",
        "\n",
        "We want to modify the network such that we extract the logits for the 10 classes from the last fully-connected layer of the network.\n",
        "\n",
        "Implement a function that replaces the current last linear layer of the pre-trained network with a new linear layer that has $O$ units ($O$ represents the number of classes in our dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk8Z1q0kuo02"
      },
      "outputs": [],
      "source": [
        "def replace_last_layer(network, O=10):\n",
        "    # replace the last linear layer with a new layer\n",
        "    ...\n",
        "\n",
        "    return network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_326vlPUWLie"
      },
      "source": [
        "#### Test 4: Last Layer Dimensions\n",
        "\n",
        "This test ensures that the function return a network having the correct number of input and output units in the last layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Da0LDf91WLie"
      },
      "outputs": [],
      "source": [
        "O = 6\n",
        "\n",
        "_test_network = replace_last_layer(network_copy, O=O)\n",
        "assert _test_network[-1].out_features == O\n",
        "assert _test_network[-1].in_features == 400"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGrsmSxAWLif"
      },
      "source": [
        "#### Task 2.10: Network Fine-Tuning with Frozen Layers\n",
        "\n",
        "Create a network that has feature layers frozen with $10$ output units.\n",
        "Fine-tune the created network for 2 epochs on our digits data (`trainloader_digits, validloader_digits`) using the previous function, and a smaller learning rate of $\\eta=0.001$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ha97BUrvV54"
      },
      "outputs": [],
      "source": [
        "network_with_frozen_layers = ...\n",
        "_ = training_loop(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmxcq6CRqjmd"
      },
      "source": [
        "#### Test 5: Frozen Layers\n",
        "\n",
        "Check that all layers of the fine-tuned network that contain weights and biases (except for the last fully-connected layer) have not been modified by the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HDmzCpZqjmd"
      },
      "outputs": [],
      "source": [
        "for i in range(len(initial_network)-1):\n",
        "    if hasattr(initial_network[i], \"weight\"):\n",
        "        assert torch.allclose(initial_network[i].weight, network_with_frozen_layers[i].weight)\n",
        "    if hasattr(initial_network[i], \"bias\"):\n",
        "        assert torch.allclose(initial_network[i].bias, network_with_frozen_layers[i].bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utiFcXqBqjme"
      },
      "source": [
        "#### Task 2.11: Test Set Predictions\n",
        "\n",
        "Go over the validation set of the Digits dataset once again and extract predictions of the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfKrMVJVqjmf"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for x, t in validloader_digits:\n",
        "        # put data onto device\n",
        "        ...\n",
        "        # extract logits from the network\n",
        "        ...\n",
        "        # obtain predicted class\n",
        "        predicted_class = ...\n",
        "\n",
        "        # store predicted class\n",
        "        predictions.extend(predicted_class.cpu().tolist())\n",
        "\n",
        "targets = [t for _,t in validset_digits]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjruuiNnWLif"
      },
      "source": [
        "#### Task 2.12: Confusion Matrix Plotting\n",
        "\n",
        "Finally, we want to plot the confusion matrix of the validation set.\n",
        "For this, we can make use of the `sklearn.metrics.confusion_matrix` to compute the confusion matrix.\n",
        "You can utilize `sklearn.metrics.ConfusionMatrixDisplay` for displaying the confusion matrix, or `pyplot.imshow` and adding the according labels.\n",
        "\n",
        "Note:\n",
        "\n",
        "  * The documentation for the confusion matrix can be found here: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
        "  * The interface and an example for the `ConfusionMatrixDisplay` can be found here: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvUPMZ03vyyt"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# get the target class names\n",
        "classes = trainset_digits.classes\n",
        "\n",
        "# compute confusion matrix\n",
        "matrix = confusion_matrix(...) # Use predictions and target from the fine-tuned network\n",
        "\n",
        "# plot confusion matrices\n",
        "plot_conf_matrix1 = ConfusionMatrixDisplay(..., display_labels=classes)\n",
        "plot_conf_matrix1.plot(xticks_rotation = \"vertical\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "dKg8Xm0Y8_OI"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
