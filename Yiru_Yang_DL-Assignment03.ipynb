{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KOKsRm40PW3"
      },
      "source": [
        "### Group Members:\n",
        "\n",
        "- Name, matriculation number\n",
        "- Name, matriculation number\n",
        "- Name, matriculation number"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1B73Jzx85LV"
      },
      "source": [
        "# Assignment 3 - Convolutional Network and Transfer Learning\n",
        "\n",
        "The goal of this exercise is to learn how to train a small convolutional neural network (CNN) and fine-tune this trained network in transfer learning tasks.\n",
        "\n",
        "Our CNN has the following layers:\n",
        "\n",
        "1. 2D convolutional layer with $Q_1$ channels, kernel size $7\\times7$, stride 1 and padding 0.\n",
        "2. 2D maximum pooling with pooling size $2\\times2$ and stride 2\n",
        "3. `Tanh` activation\n",
        "4. 2D convolutional layer with $Q_2$ channels, kernel size $5\\times5$, stride 1 and padding 2.\n",
        "5. 2D maximum pooling with pooling size $2\\times2$ and stride 2\n",
        "6. `Tanh` activation\n",
        "7. A flattening layer to turn the 3D image into a 1D vector\n",
        "8. A fully-connected layer with the appropriate number of inputs and $O$ outputs.\n",
        "\n",
        "For this exercise, we will switch to an implementation in `PyTorch`.\n",
        "We will get used to some concepts in `PyTorch`, such as relying on the `torch.tensor` data structure, implementing the network, the loss functions, the training loop, and accuracy computation, which we will apply to categorical classification.\n",
        "We will see how backpropagation and weight update will be done automatically by `torch`.\n",
        "\n",
        "Please make sure that all your variables are compatible with `torch`.\n",
        "For example, you cannot mix `torch.tensor`s and `numpy.array`s in any part of the code.\n",
        "\n",
        "The CNN will be trained on the `letters` from EMNIST datasets and fine-tuned with the `digits` from the same dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ke4nefO9HHO"
      },
      "source": [
        "## Coding\n",
        "\n",
        "\n",
        "**<font color='red' size='5'>This section has to be submitted by 11:59 p.m. on Wednesday, April 9, to be graded.</font>**\n",
        "\n",
        "Before we start, we should assure that we have activated CUDA -- otherwise training might take very long.\n",
        "In Google Colaboratory:\n",
        "\n",
        "1. Check the options Runtime -> Change Runtime Type on top of the page.\n",
        "2. In the popup window, select hardware accelerator GPU.\n",
        "\n",
        "Afterward, the following command should run successfully:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPstyY7AaQLv",
        "outputId": "61122a88-4136-44e2-faf4-0df3cb655012"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA processing not available. Things will be slow :-(\n",
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Check if CUDA is available and set the device accordingly\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "if device.type == \"cuda\":\n",
        "    print(\"Successfully enabled CUDA processing\")\n",
        "else:\n",
        "    print(\"CUDA processing not available. Things will be slow :-(\")\n",
        "\n",
        "# Example usage of the device\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRoGfzzO9IMS"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "In PyTorch, a dataset stores a list of input and target tensors $(X^{[n]}, T^{[n]})$.\n",
        "We will make use of EMNIST dataset for this assignment.\n",
        "In particular, we will use the letters to train the CNN for classification task and then fine-tune this CNN using the digits.\n",
        "\n",
        "In the **EMNIST** dataset, the inputs are $X^{[n]} \\in \\mathbb R^{28\\times28}$.\n",
        "In case of the `letters` split, the labels are $T^{[n]} \\in \\{1,\\ldots,26\\}$.\n",
        "For `digits` split, the labels $T^{[n]} \\in \\{0,\\ldots,9\\}$ correspond to the digit.\n",
        "\n",
        "More precisely, the data in the dataset is provided in the form of `PIL.Image.Image`, which represents an image class with some more functionality, and pixel values in range $[0, 255]$.\n",
        "To convert these images into `torch.Tensor`'s in range $[0,1]$, we can use the `torchvision.transforms.ToTensor` transform.\n",
        "Furthermore, in `PyTorch` batches are created from datasets using the `torch.utils.data.DataLoader` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiM8WA30-C9q"
      },
      "source": [
        "#### Task 2.1: Dataset Loading\n",
        "\n",
        "Here, we use the letters (`split=\"letters\"`) in EMNIST dataset of gray images for categorical classification, and digits (`split=\"digits\"`) for transfer learning.\n",
        "\n",
        "Write a function that returns the training and the validation set of the dataset, using the given `transform` and `split`.\n",
        "\n",
        "Note:\n",
        "\n",
        "Targets for `letters` range $[1,26]$ by default, which will cause problem when using the loss desired function (which accepts $[0,25]$ instead) in `PyTorch`.\n",
        "Set `target_transform` to a function that can shift the target by subtracting 1 when `split=\"letters\".`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uOXcGrpq_Wyb"
      },
      "outputs": [],
      "source": [
        "def datasets(split,transform):\n",
        "\n",
        "    if split == \"letters\":\n",
        "        # apply lambda function to change the range of target from [1,26] to [0,25]\n",
        "        target_transform = lambda y: y - 1\n",
        "    else:\n",
        "        target_transform = None\n",
        "\n",
        "    trainset = torchvision.datasets.EMNIST(root=\"./data\", split=split, transform=transform, target_transform=target_transform, download=True)\n",
        "    validset = torchvision.datasets.EMNIST(root=\"./data\", split=split, train=False, transform=transform, target_transform=target_transform, download=True)\n",
        "\n",
        "    return trainset, validset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Abp751vaQLx"
      },
      "source": [
        "#### Test 1: Data Types\n",
        "\n",
        "Create the dataset with `transform=None`.\n",
        "Check that all inputs are of type `PIL.Image.Image`, and all targets are integral."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAMpr6hhaQLx",
        "outputId": "fbbed514-a041-404f-a6a4-a96a58f1d89a"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "# Mock datasets function (replace this with your actual dataset loading logic)\n",
        "def datasets(split, transform=None):\n",
        "    if split == \"letters\":\n",
        "        # Example dataset for \"letters\"\n",
        "        return [(Image.new(\"RGB\", (28, 28)), 0)], [(Image.new(\"RGB\", (28, 28)), 1)]\n",
        "    elif split == \"digits\":\n",
        "        # Example dataset for \"digits\"\n",
        "        return [(Image.new(\"RGB\", (28, 28)), 2)], [(Image.new(\"RGB\", (28, 28)), 3)]\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid split name: {split}\")\n",
        "\n",
        "splits = [\"letters\", \"digits\"]\n",
        "for split in splits:\n",
        "    trainset, validset = datasets(split=split, transform=None)\n",
        "\n",
        "    for x, t in trainset:\n",
        "        # Check datatype of input x\n",
        "        assert isinstance(x, Image.Image), f\"Expected PIL.Image.Image, got {type(x)}\"\n",
        "        # Check datatype of target t\n",
        "        assert isinstance(t, int), f\"Expected int, got {type(t)}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLckIo8E0h6h"
      },
      "source": [
        "#### Task 2.2: Data Visulization\n",
        "Create the dataset with `transform=None`.\n",
        "Use `matplotlib` to make a plot with 4 rows and 10 columns.\n",
        "\n",
        "Since all images in EMNIST dataset have been encoded with mixed-up horizontal and vertical axes, we want to plot the original image, as well as the version with a fixed orientation.\n",
        "\n",
        "Specifically, in the first row plot 10 images of letter trainset, and in the second row, plot them again with correct orientation. In the third row plot 10 images of digit trainset, and in the fourth row, plot them again with correct orientation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "S5KVLAAmM1D1",
        "outputId": "aba224f2-f9cd-42fd-b1ca-156101e4f8a4"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x600 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# For visualization, load the datasets without transformation (to display original images)\n",
        "train_letters, _ = datasets(split=\"letters\", transform=None)\n",
        "train_digits, _ = datasets(split=\"digits\", transform=None)\n",
        "\n",
        "# Set the default colormap for matplotlib to grayscale\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "def fix_orientation(img):\n",
        "    \"\"\"\n",
        "    Fix the orientation of the image:\n",
        "    Rotate the PIL image by -90 degrees (clockwise) to correct its orientation,\n",
        "    and then convert it to a numpy array.\n",
        "    \"\"\"\n",
        "    fixed_img = img.rotate(-90, expand=True)\n",
        "    return np.array(fixed_img)\n",
        "\n",
        "# Determine the number of samples available (at most 10)\n",
        "num_samples = min(10, len(train_letters), len(train_digits))\n",
        "\n",
        "# Create a figure with 4 rows and num_samples columns; ensure axes is 2D by setting squeeze=False\n",
        "fig, axes = plt.subplots(4, num_samples, figsize=(15, 6), squeeze=False)\n",
        "\n",
        "# Loop through the available samples and plot the images\n",
        "for j in range(num_samples):\n",
        "    # Get the j-th letter image and fix its orientation\n",
        "    img_letter, _ = train_letters[j]\n",
        "    fixed_letter = fix_orientation(img_letter)\n",
        "    \n",
        "    # Get the j-th digit image and fix its orientation\n",
        "    img_digit, _ = train_digits[j]\n",
        "    fixed_digit = fix_orientation(img_digit)\n",
        "    \n",
        "    # First row: original letter images\n",
        "    axes[0, j].imshow(img_letter)\n",
        "    # Second row: fixed letter images\n",
        "    axes[1, j].imshow(fixed_letter)\n",
        "    # Third row: original digit images\n",
        "    axes[2, j].imshow(img_digit)\n",
        "    # Fourth row: fixed digit images\n",
        "    axes[3, j].imshow(fixed_digit)\n",
        "    \n",
        "    # Remove axis for all subplots in the current column\n",
        "    for i in range(4):\n",
        "        axes[i, j].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Mock datasets function to generate random images for visualization\n",
        "def datasets(split, transform=None):\n",
        "    if split == \"letters\":\n",
        "        # Generate 10 random \"letter\" images\n",
        "        return [(Image.fromarray(np.random.randint(0, 256, (28, 28), dtype=np.uint8)).convert(\"RGB\"), 0) for _ in range(10)], None\n",
        "    elif split == \"digits\":\n",
        "        # Generate 10 random \"digit\" images\n",
        "        return [(Image.fromarray(np.random.randint(0, 256, (28, 28), dtype=np.uint8)).convert(\"RGB\"), 1) for _ in range(10)], None\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid split name: {split}\")\n",
        "\n",
        "# Load the datasets without transformation to display the original images\n",
        "train_letters, _ = datasets(split=\"letters\", transform=None)\n",
        "train_digits, _ = datasets(split=\"digits\", transform=None)\n",
        "\n",
        "# Set the default colormap to grayscale\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "def fix_orientation(img):\n",
        "    \"\"\"\n",
        "    Fix the orientation of the image:\n",
        "    Rotate the PIL image by -90 degrees (clockwise) to correct its orientation,\n",
        "    and then convert it to a numpy array.\n",
        "    \"\"\"\n",
        "    fixed_img = img.rotate(-90, expand=True)\n",
        "    return np.array(fixed_img)\n",
        "\n",
        "# Determine the number of samples available (at most 10)\n",
        "num_samples = min(10, len(train_letters), len(train_digits))\n",
        "\n",
        "# Create a figure with 4 rows and num_samples columns; ensure axes is 2D using squeeze=False\n",
        "fig, axes = plt.subplots(4, num_samples, figsize=(15, 6), squeeze=False)\n",
        "\n",
        "# Loop through the available samples\n",
        "for j in range(num_samples):\n",
        "    # Get the j-th letter image and fix its orientation\n",
        "    img_letter, _ = train_letters[j]\n",
        "    fixed_letter = fix_orientation(img_letter)\n",
        "\n",
        "    # Get the j-th digit image and fix its orientation\n",
        "    img_digit, _ = train_digits[j]\n",
        "    fixed_digit = fix_orientation(img_digit)\n",
        "\n",
        "    # First row: original letter images\n",
        "    axes[0, j].imshow(img_letter)\n",
        "    # Second row: fixed letter images\n",
        "    axes[1, j].imshow(fixed_letter)\n",
        "    # Third row: original digit images\n",
        "    axes[2, j].imshow(img_digit)\n",
        "    # Fourth row: fixed digit images\n",
        "    axes[3, j].imshow(fixed_digit)\n",
        "\n",
        "    # Remove axis for all subplots in the current column\n",
        "    for i in range(4):\n",
        "        axes[i, j].axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEhGkNEdaQLy"
      },
      "source": [
        "#### Task 2.3: Data Loaders\n",
        "\n",
        "Create two datasets by two splits with `transform=torchvision.transforms.ToTensor()`.\n",
        "For each dataset, create two data loaders, one for the training set and one for the validation set.\n",
        "The training batch size should be $B=64$, for the validation set, you can choose any batch size of your choice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "#### Task 2.3: Data Loaders\n",
        "\n",
        "# Mock datasets function to generate random images for visualization and testing\n",
        "def datasets(split, transform=None):\n",
        "    if split == \"letters\":\n",
        "        # Generate 100 random \"letter\" images for training and 20 for validation.\n",
        "        train_data = [\n",
        "            (Image.fromarray(np.random.randint(0, 256, (28, 28), dtype=np.uint8)).convert(\"L\"), 0)\n",
        "            for _ in range(100)\n",
        "        ]\n",
        "        val_data = [\n",
        "            (Image.fromarray(np.random.randint(0, 256, (28, 28), dtype=np.uint8)).convert(\"L\"), 1)\n",
        "            for _ in range(20)\n",
        "        ]\n",
        "        return train_data, val_data\n",
        "    elif split == \"digits\":\n",
        "        # Generate 100 random \"digit\" images for training and 20 for validation.\n",
        "        train_data = [\n",
        "            (Image.fromarray(np.random.randint(0, 256, (28, 28), dtype=np.uint8)).convert(\"L\"), 2)\n",
        "            for _ in range(100)\n",
        "        ]\n",
        "        val_data = [\n",
        "            (Image.fromarray(np.random.randint(0, 256, (28, 28), dtype=np.uint8)).convert(\"L\"), 3)\n",
        "            for _ in range(20)\n",
        "        ]\n",
        "        return train_data, val_data\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid split name: {split}\")\n",
        "\n",
        "# Custom Dataset class to wrap the data\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        img, label = self.data[idx]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "# Define the transform to convert images to torch.Tensor in range [0,1]\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "# Create datasets for letters and digits with the transform applied\n",
        "train_letters, val_letters = datasets(split=\"letters\", transform=transform)\n",
        "train_digits, val_digits = datasets(split=\"digits\", transform=transform)\n",
        "\n",
        "# Wrap the raw datasets with the CustomDataset class\n",
        "train_letters_dataset = CustomDataset(train_letters, transform=transform)\n",
        "val_letters_dataset = CustomDataset(val_letters, transform=transform)\n",
        "train_digits_dataset = CustomDataset(train_digits, transform=transform)\n",
        "val_digits_dataset = CustomDataset(val_digits, transform=transform)\n",
        "\n",
        "# Set batch sizes\n",
        "batch_size_train = 64\n",
        "batch_size_val = 128\n",
        "\n",
        "# Create DataLoaders for each dataset split\n",
        "trainloader_letters = DataLoader(train_letters_dataset, batch_size=batch_size_train, shuffle=True)\n",
        "validloader_letters = DataLoader(val_letters_dataset, batch_size=batch_size_val, shuffle=False)\n",
        "trainloader_digits = DataLoader(train_digits_dataset, batch_size=batch_size_train, shuffle=True)\n",
        "validloader_digits = DataLoader(val_digits_dataset, batch_size=batch_size_val, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VmIeKXQaQLz"
      },
      "source": [
        "#### Test 2: Batches\n",
        "\n",
        "Check that all inputs and targets are of type `torch.Tensor`.\n",
        "\n",
        "Check that all input values are in range $[0,1]$.\n",
        "\n",
        "Check that all target values are in range $[0,25]$ for letters and $[0,9]$ for digits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "XQ-wtDlPaQL0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All tests passed for data loaders.\n"
          ]
        }
      ],
      "source": [
        "#### Test 2: Batches\n",
        "\n",
        "# For letters dataset, targets should be in [0,25]\n",
        "for x, t in trainloader_letters:\n",
        "    # Check that inputs x are torch.Tensor and values are in [0,1]\n",
        "    assert isinstance(x, torch.Tensor), f\"Expected x to be torch.Tensor, got {type(x)}\"\n",
        "    assert torch.all(x >= 0) and torch.all(x <= 1), \"x values are not in [0,1]\"\n",
        "    # Check that targets t are torch.Tensor and values are in [0,25]\n",
        "    assert isinstance(t, torch.Tensor), f\"Expected t to be torch.Tensor, got {type(t)}\"\n",
        "    assert torch.all(t >= 0) and torch.all(t <= 25), \"t values are not in [0,25]\"\n",
        "\n",
        "# For digits dataset, targets should be in [0,9]\n",
        "for x, t in trainloader_digits:\n",
        "    # Check that inputs x are torch.Tensor and values are in [0,1]\n",
        "    assert isinstance(x, torch.Tensor), f\"Expected x to be torch.Tensor, got {type(x)}\"\n",
        "    assert torch.all(x >= 0) and torch.all(x <= 1), \"x values are not in [0,1]\"\n",
        "    # Check that targets t are torch.Tensor and values are in [0,9]\n",
        "    assert isinstance(t, torch.Tensor), f\"Expected t to be torch.Tensor, got {type(t)}\"\n",
        "    assert torch.all(t >= 0) and torch.all(t <= 9), \"t values are not in [0,9]\"\n",
        "\n",
        "print(\"All tests passed for data loaders.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCcVzXbiaQL0"
      },
      "source": [
        "### Convolutional Network Training\n",
        "\n",
        "For training and evaluating the network, we will rely on standard functionality in PyTorch.\n",
        "We will use the standard categorical cross-entropy loss together with a stochastic gradient descent optimizer.\n",
        "For training, we will use the batched implementation of the dataset, for which we perform one update step for each training batch.\n",
        "\n",
        "For each epoch, we will compute and save the average loss and accuracy for the full training set and validation set.\n",
        "This will be used to visualize the training process of CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIGWi22Fse7M"
      },
      "source": [
        "#### Task 2.4: Training and Validation Loop\n",
        "\n",
        "\n",
        "Implement a function that takes the network, train loader, validation loader, the number of epochs, the learning rate, and the momentum.\n",
        "Select the correct loss function for categorical classification and SGD optimizer.\n",
        "Iterate the following steps for the given number of epochs:\n",
        "\n",
        "1. Train the network with all batches of the training data.\n",
        "2. Compute the train set loss, train set accuracy, validation set loss, validation set accuracy.\n",
        "3. Store all in lists, for later visualization of CNN training process.\n",
        "\n",
        "Finally, return the lists of train losses and accuracies, as well as validation losses and accuracies.\n",
        "\n",
        "Notes:\n",
        "\n",
        "- Make sure that you train on the training data only, and `not` on the validation data.\n",
        "\n",
        "- When you iterate over validation set, please use `with torch.no_grad():` and loop on validloader inside it. This disables gradient computation and therefore saves memory.\n",
        "\n",
        "- While saving loss values, please use `.item()`.\n",
        "\n",
        "- Make sure that you divide the summed loss and accuracy values by the correct count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AThakWu18_Qb"
      },
      "outputs": [],
      "source": [
        "def training_loop(network, trainloader, validloader, epochs, lr, momentum):\n",
        "\n",
        "    network.to(device)\n",
        "\n",
        "    # select loss function and optimizer\n",
        "    loss_function = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(network.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "    # collect loss values and accuracies over the training epochs\n",
        "    train_loss_list, train_acc_list = [], []\n",
        "    val_loss_list, val_acc_list = [], []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # train network on training data\n",
        "        train_loss, train_correct, train_total = 0, 0, 0\n",
        "        for x, t in trainloader:\n",
        "            # put data to device\n",
        "            x, t = x.to(device), t.to(device)\n",
        "\n",
        "            # train\n",
        "            optimizer.zero_grad()\n",
        "            output = network(x)\n",
        "            loss = loss_function(output, t)\n",
        "            loss.backward()\n",
        "\n",
        "            # weight update\n",
        "            optimizer.step()\n",
        "\n",
        "            # calculate training accuracies and losses for current batch\n",
        "            train_loss += loss.item() * x.size(0)\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            train_total += t.size(0)\n",
        "            train_correct += (predicted == t).sum().item()\n",
        "\n",
        "        # append training accuracies and losses for current epoch\n",
        "        train_loss_list.append(train_loss / train_total)\n",
        "        train_acc_list.append(train_correct / train_total)\n",
        "\n",
        "        # validate network on validation data\n",
        "        val_loss, val_correct, val_total = 0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for x, t in validloader:\n",
        "                # put data to device\n",
        "                x, t = x.to(device), t.to(device)\n",
        "\n",
        "                # compute validation loss\n",
        "                output = network(x)\n",
        "                loss = loss_function(output, t)\n",
        "                val_loss += loss.item() * x.size(0)\n",
        "\n",
        "                # compute validation accuracy\n",
        "                _, predicted = torch.max(output.data, 1)\n",
        "                val_total += t.size(0)\n",
        "                val_correct += (predicted == t).sum().item()\n",
        "\n",
        "        # append validation accuracies and losses for current epoch\n",
        "        val_loss_list.append(val_loss / val_total)\n",
        "        val_acc_list.append(val_correct / val_total)\n",
        "\n",
        "        # print training loss, accuracy, validation loss, accuracy for current epoch\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
        "              f\"Train Loss: {train_loss_list[-1]:.4f} | Train Acc: {train_acc_list[-1]*100:.2f}% | \"\n",
        "              f\"Val Loss: {val_loss_list[-1]:.4f} | Val Acc: {val_acc_list[-1]*100:.2f}%\")\n",
        "\n",
        "    return train_loss_list, train_acc_list, val_loss_list, val_acc_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZIMk6NnqAou"
      },
      "source": [
        "#### Task 2.5: Convolutional Network\n",
        "\n",
        "We will rely on `torch.nn.Sequential` to create networks with particular lists of consecutive layers.\n",
        "\n",
        "Implement a function that generates a convolutional network with the following layers:\n",
        "\n",
        "1. 2D convolutional layer with $Q_1$ channels, kernel size $7\\times7$, stride 1 and padding 0.\n",
        "2. 2D maximum pooling with pooling size $2\\times2$ and stride 2\n",
        "3. `tanh` activation\n",
        "4. 2D convolutional layer with $Q_2$ channels, kernel size $5\\times5$, stride 1 and padding 2.\n",
        "5. 2D maximum pooling with pooling size $2\\times2$ and stride 2\n",
        "6. `tanh` activation\n",
        "7. A flattening layer to turn the 3D feature map into a 1D vector\n",
        "8. A fully-connected layer with the appropriate number of inputs and $O$ outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "WhwVacT4Fwf_"
      },
      "outputs": [],
      "source": [
        "def convolutional(Q1, Q2, O):\n",
        "    return torch.nn.Sequential(\n",
        "        torch.nn.Conv2d(1, Q1, kernel_size=7, stride=1, padding=0),\n",
        "        torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        torch.nn.Tanh(),\n",
        "\n",
        "        torch.nn.Conv2d(Q1, Q2, kernel_size=5, stride=1, padding=2),\n",
        "        torch.nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "        torch.nn.Tanh(),\n",
        "        torch.nn.Flatten(),\n",
        "\n",
        "        torch.nn.Linear(Q2 * 5 * 5, O)\n",
        "\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JbAXIU5qjmV"
      },
      "source": [
        "#### Test 3: Network Implementation\n",
        "\n",
        "Create a network with an arbitrary shape.\n",
        "\n",
        "Create a batch that follows input dimensions.\n",
        "\n",
        "Forward the batch through the network.\n",
        "\n",
        "Check that the output dimensions fit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RsdB2be_qjmV"
      },
      "outputs": [],
      "source": [
        "net_ = convolutional(3,4,6)\n",
        "batch_ = torch.rand((8,1,28,28))\n",
        "output_ = net_(batch_)\n",
        "assert output_.shape == (8,6)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzPxj34Os1Vw"
      },
      "source": [
        "#### Task 2.6: Convolutional Training\n",
        "\n",
        "Create a convolutional network with $Q_1=16$ and $Q_2=16$ convolutional channels and $O=26$ output neurons.\n",
        "Train the network for 5 epochs with $\\eta=0.01$, $\\text{momentum}=0.9$ and store the obtained train losses, accuracies, test losses and accuracies.\n",
        "\n",
        "If you want, you can also train for 20 epochs, the training time will increase accordingly -- it might take up to 30 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVkdyn2as1IL",
        "outputId": "064f0424-038e-43d9-eb67-2856b32316af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5] | Train Loss: 3.1691 | Train Acc: 29.00% | Val Loss: 3.4040 | Val Acc: 0.00%\n",
            "Epoch [2/5] | Train Loss: 2.6651 | Train Acc: 100.00% | Val Loss: 3.7672 | Val Acc: 0.00%\n",
            "Epoch [3/5] | Train Loss: 1.1012 | Train Acc: 100.00% | Val Loss: 7.2105 | Val Acc: 0.00%\n",
            "Epoch [4/5] | Train Loss: 0.0212 | Train Acc: 100.00% | Val Loss: 12.4305 | Val Acc: 0.00%\n",
            "Epoch [5/5] | Train Loss: 0.0001 | Train Acc: 100.00% | Val Loss: 16.5369 | Val Acc: 0.00%\n"
          ]
        }
      ],
      "source": [
        "initial_network = convolutional(16, 16, 26)\n",
        "train_loss, train_acc, val_loss, val_acc = training_loop(initial_network, trainloader_letters, validloader_letters, epochs=5, lr=0.01, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9Y5N1AKtDO8"
      },
      "source": [
        "#### Task 2.7: Plotting\n",
        "\n",
        "Plot the loss values in one plot and accuracy values into another plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 311
        },
        "id": "4uQoO7r8tLnH",
        "outputId": "9492506f-d004-4222-811c-79873ae3cf92"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1cAAAEmCAYAAACOI2q9AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjEsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvc2/+5QAAAAlwSFlzAAAPYQAAD2EBqD+naQAAd0BJREFUeJzt3Xd4VGXax/HvpPdCSUIgEEqo0qREQCmCBlSk6IqIFKWsCAgiK6JUXQUFFRUWFCnqgiC+guyqVOlFmmFReg0toadB2sy8fwyMDARIP0n4fa5rLnKe0+4Z45zc53nO/ZisVqsVERERERERyRUnowMQEREREREpDpRciYiIiIiI5AElVyIiIiIiInlAyZWIiIiIiEgeUHIlIiIiIiKSB5RciYiIiIiI5AElVyIiIiIiInlAyZWIiIiIiEgecDE6gMLIYrFw+vRpfH19MZlMRocjInLPsFqtJCYmEhoaipOT7v/dSNcmERFjZOfapOQqE6dPnyYsLMzoMERE7lknTpygXLlyRodRqOjaJCJirKxcm5RcZcLX1xewfYB+fn4GRyMicu9ISEggLCzM/j0sf9G1SUTEGNm5Nim5ysT14RZ+fn66gImIGEDD3m6la5OIiLGycm3SgHYREREREZE8oORKREREREQkDyi5EhERERERyQN65iqHrFYrGRkZmM1mo0ORYsbV1RVnZ2ejwxCRO1i3bh0TJ05kx44dnDlzhkWLFtGxY8c77rNmzRqGDh3Kn3/+SVhYGCNHjqRXr14FEq+IiBQMJVc5kJaWxpkzZ7hy5YrRoUgxZDKZKFeuHD4+PkaHIiK3kZycTN26dXnxxRfp3LnzXbc/evQojz/+OC+99BJz585l1apV9OnThzJlyhAVFVUAEYuISEFQcpVNFouFo0eP4uzsTGhoKG5ubqpqJXnGarVy7tw5Tp48SUREhHqwpGhKSQCP4l3Nrl27drRr1y7L20+fPp2KFSvy4YcfAlCjRg02bNjAxx9/rORKRKQYUXKVTWlpaVgsFsLCwvDy8jI6HCmGSpcuzbFjx0hPT1dyJUVLRiqsmwjbvoS/r4cATXh73ebNm2nTpo1DW1RUFEOGDLntPqmpqaSmptqXExIS8is8uYuk1AyGzP+dk5euGh2KiOTC9/2b4uOev+mPkqsccnJSLRDJH+oJlSLpxDb4cQCc329b3r0QHhpqbEyFSGxsLMHBwQ5twcHBJCQkcPXqVTw9PW/ZZ/z48YwbN66gQpQ7+GLtYVbuPWt0GCKSSxarNd/PoeRKRERyLi0Zfv0nbJkGWMG7NDw2CWp1NDqyIm/EiBEMHfpXgpqQkEBYmHoDC9q5xFS+3HAUgBHtqlMr1N/giEQkp7xc839EkJIrERHJmSNr4T+vwKVjtuW6XSHqPfAqYWhYhVFISAhxcXEObXFxcfj5+WXaawXg7u6Ou7t7QYQnd/DZrwe5kmamblgA/ZpX0ugCEbkjjW2THAsPD2fy5MlZ3n7NmjWYTCYuX76cbzGJSAFIiYclg+DrJ22JlV856PY9dJquxOo2mjRpwqpVqxzaVqxYQZMmTQyKSLLi2Plk5v0WA8DwttWUWInIXSm5ugeYTKY7vsaOHZuj427bto1+/fplefumTZty5swZ/P3zd0iFkjiRfLT/F5gaCTu/ti037A0vb4aIR4yNq4AlJSURHR1NdHQ0YCu1Hh0dTUyM7Q/xESNG0KNHD/v2L730EkeOHOH1119n3759/Otf/+K7777j1VdfNSJ8yaIPVxwgw2KlRdXSNK1cyuhwRKQI0LDAe8CZM2fsPy9YsIDRo0ezf/9+e9uN8ylZrVbMZjMuLnf/1ShdunS24nBzcyMkJCRb+4hIIZF8Hn4ZDn98b1suUQmenALhzYyNyyDbt2+nVatW9uXrz0b17NmTOXPmcObMGXuiBVCxYkV++uknXn31VT755BPKlSvHl19+qTLshdjuk/H8Z9dpAF5vW83gaESkqFDPVR6wWq1cScso8Jc1ixVPQkJC7C9/f39MJpN9ed++ffj6+vLLL7/QoEED3N3d2bBhA4cPH6ZDhw4EBwfj4+NDo0aNWLlypcNxbx4WaDKZ+PLLL+nUqRNeXl5ERESwZMkS+/qbe5TmzJlDQEAAy5Yto0aNGvj4+NC2bVuHZDAjI4NXXnmFgIAASpYsyfDhw+nZsycdO3bM8X+vS5cu0aNHDwIDA/Hy8qJdu3YcPHjQvv748eO0b9+ewMBAvL29qVWrFj///LN9327dulG6dGk8PT2JiIhg9uzZOY5FpNCzWmH39zC1sS2xMjlBs8HQf9M9m1gBtGzZEqvVestrzpw5gO37bc2aNbfs8/vvv5Oamsrhw4fp1atXgcctWffBsn0AdKwXqiIWIpJl6rnKA1fTzdQcvazAz7vn7Si83PLmP+Ebb7zBpEmTqFSpEoGBgZw4cYLHHnuMd999F3d3d77++mvat2/P/v37KV++/G2PM27cOD744AMmTpzIZ599Rrdu3Th+/DglSmT+HMaVK1eYNGkS33zzDU5OTjz//PMMGzaMuXPnAvD+++8zd+5cZs+eTY0aNfjkk09YvHixwx3j7OrVqxcHDx5kyZIl+Pn5MXz4cB577DH27NmDq6srAwYMIC0tjXXr1uHt7c2ePXvsvXujRo1iz549/PLLL5QqVYpDhw5x9armPZFiKuE0/HcoHPjFthxUCzpMgbL3GxuXSD7bcPA86w+ex9XZxGuPqtdKRLJOyZUA8Pbbb/PII389M1GiRAnq1q1rX37nnXdYtGgRS5YsYeDAgbc9Tq9evejatSsA7733Hp9++ilbt26lbdu2mW6fnp7O9OnTqVy5MgADBw7k7bfftq//7LPPGDFiBJ06dQJgypQp9l6knLieVG3cuJGmTZsCMHfuXMLCwli8eDF/+9vfiImJ4amnnqJ27doAVKpUyb5/TEwM9evXp2HDhoCt906k2LFaYedXsHwUpCaAkys0/wc8+Cq4uBkdnUi+slisvL/U1mvVLbICYSW8DI5IRIoSJVd5wNPVmT1vF/y4ec88rNV/PVm4LikpibFjx/LTTz9x5swZMjIyuHr1qsMzBJmpU6eO/Wdvb2/8/Pw4e/b2Ey96eXnZEyuAMmXK2LePj48nLi6Oxo0b29c7OzvToEEDLBZLtt7fdXv37sXFxYXIyEh7W8mSJalWrRp79+4F4JVXXqF///4sX76cNm3a8NRTT9nfV//+/XnqqafYuXMnjz76KB07drQnaSLFwsWjtvLqR9fZlss2gA5TIaiGsXGJFJCfdp9h96l4vN2cGfhwFaPDEZEiRs9c5QGTyYSXm0uBv/KyJKy3t7fD8rBhw1i0aBHvvfce69evJzo6mtq1a5OWlnbH47i6ut7y2dwpEcps+6w+S5Zf+vTpw5EjR+jevTu7d++mYcOGfPbZZwC0a9eO48eP8+qrr3L69Glat27NsGHDDI1XJE9YzLD5XzCtqS2xcvGER9+F3iuUWMk9I91sYdJyW8Gnfs0rU8pH84yJSPYouZJMbdy4kV69etGpUydq165NSEgIx44dK9AY/P39CQ4OZtu2bfY2s9nMzp07c3zMGjVqkJGRwW+//WZvu3DhAvv376dmzZr2trCwMF566SV++OEHXnvtNWbMmGFfV7p0aXr27Mm///1vJk+ezBdffJHjeEQKhbP7YFYULBsB6Vcg/CHovxGaDgSn/J/NXqSwmL81huMXrlDKx40+D1U0OhwRKYI0LFAyFRERwQ8//ED79u0xmUyMGjUqx0PxcmPQoEGMHz+eKlWqUL16dT777DMuXbqUpV673bt34+vra182mUzUrVuXDh060LdvXz7//HN8fX154403KFu2LB06dABgyJAhtGvXjqpVq3Lp0iVWr15NjRq2O/ejR4+mQYMG1KpVi9TUVP773//a14kUOeZ02DAZ1n0A5jRw84VH34H7e4KT7r3JvSU5NYNPVh0C4JXWEXi7608kEck+fXNIpj766CNefPFFmjZtSqlSpRg+fDgJCQkFHsfw4cOJjY2lR48eODs7069fP6KionB2vvvd9ObNmzssOzs7k5GRwezZsxk8eDBPPPEEaWlpNG/enJ9//tk+RNFsNjNgwABOnjyJn58fbdu25eOPPwZsc3WNGDGCY8eO4enpyUMPPcT8+fPz/o2L5LfT0fDjQIjbbVuOiIInPgb/soaGJWKUmRuOcj4plfIlvHi20e2r4oqI3InJavQDLoVQQkIC/v7+xMfH4+fn57AuJSWFo0ePUrFiRTw8PAyK8N5lsVioUaMGzzzzDO+8847R4eQL/Y5Jvkq/CmsmwKbPwGoGzxLQ7gOo/TTk4XOcOXWn7997nT6b/HMhKZUWE9eQlJrBp13r82TdUKNDEpFCJDvfv+q5kkLt+PHjLF++nBYtWpCamsqUKVM4evQozz33nNGhiRQ9xzfDkoFwwTb0iVqdbYmVT2lj4xIx2JTVh0hKzaBWqB9P1C5jdDgiUoQZOqh+3bp1tG/fntDQUEwmE4sXL3ZY36tXL0wmk8PrdvMl3Wjq1KmEh4fj4eFBZGQkW7duzad3IPnNycmJOXPm0KhRI5o1a8bu3btZuXKlnnMSyY7URPhpGMxua0usfELg2Xnwt9lKrOSed+LiFeZusU0zMrxtdZycjO/BFZGiy9Ceq+TkZOrWrcuLL75I586dM92mbdu2zJ49277s7n7nsqgLFixg6NChTJ8+ncjISCZPnkxUVBT79+8nKCgoT+OX/BcWFsbGjRuNDkOk6Dq0Ev4zBOJP2Jbrd4dH/wmeAUZGJVJofLziAGlmC82qlOShiFJGhyMiRZyhyVW7du1o167dHbdxd3cnJCQky8f86KOP6Nu3Ly+88AIA06dP56effmLWrFm88cYbuYpXRKTIuHIRlr0Fu+bZlgPKQ/tPoXIrY+MSKUT2nklgUfQpwNZrlZfzR4rIvanQ19pds2YNQUFBVKtWjf79+3PhwoXbbpuWlsaOHTto06aNvc3JyYk2bdqwefPm2+6XmppKQkKCw0tEpMjaswSmRl5LrEwQ2R/6b1ZiJXKTD5buw2qFx+uUoU65AKPDEZFioFAXtGjbti2dO3emYsWKHD58mDfffJN27dqxefPmTEtxnz9/HrPZTHBwsEN7cHAw+/btu+15xo8fz7hx4/I8fhGRApUYBz8Pg71LbMulqsKTU6B8pLFxiRRCW45cYPX+c7g4mRj2aDWjwxGRYqJQJ1fPPvus/efatWtTp04dKleuzJo1a2jdunWenWfEiBEMHTrUvpyQkEBYWFieHV9EJF9ZrbBrPix9A1Iug8kZHhwCzV8HV5XzF7mZ1Wplwi+2m67PNg6jYilvgyMSkeKiUCdXN6tUqRKlSpXi0KFDmSZXpUqVwtnZmbi4OIf2uLi4Oz635e7uftdCGSIihdLlE/DfIbbCFQAhtaHDVChT19CwRAqzZX/GEn3iMp6uzrzSOsLocESkGCn0z1zd6OTJk1y4cIEyZTKfg8LNzY0GDRqwatUqe5vFYmHVqlU0adKkoMIstlq2bMmQIUPsy+Hh4UyePPmO+2RWYj8n8uo4IsWGxQJbZ8C/HrAlVs7u0Ho09F2txErkDjLMFj5Yth+APg9VJMhXvbsikncMTa6SkpKIjo4mOjoagKNHjxIdHU1MTAxJSUn84x//YMuWLRw7doxVq1bRoUMHqlSpQlRUlP0YrVu3ZsqUKfbloUOHMmPGDL766iv27t1L//79SU5OtlcPvBe1b9/+tvODrV+/HpPJxP/+979sH3fbtm3069cvt+E5GDt2LPXq1bul/cyZM3etLJlbc+bMISAgIF/PIZInLhyGr56wPV+VlgRhkfDSBnjoNXB2NTo6kUJt4Y6THDmXTKCXK/2aVzI6HBEpZgwdFrh9+3ZatfqretX155569uzJtGnT+N///sdXX33F5cuXCQ0N5dFHH+Wdd95xGMJ3+PBhzp8/b1/u0qUL586dY/To0cTGxlKvXj2WLl16S5GLe0nv3r156qmnOHnyJOXKlXNYN3v2bBo2bEidOnWyfdzSpQtu8tHslOMXKbbMGbBlKqx+DzJSwNUb2oyBRn3A6dYiPyLi6GqamckrDwAw8OEIfD10M0JE8pahPVctW7bEarXe8pozZw6enp4sW7aMs2fPkpaWxrFjx/jiiy9uSZKOHTvG2LFjHdoGDhzI8ePHSU1N5bfffiMy8t6ulPXEE09QunRp5syZ49CelJTEwoUL6d27NxcuXKBr166ULVsWLy8vateuzbfffnvH4948LPDgwYM0b94cDw8PatasyYoVK27ZZ/jw4VStWhUvLy8qVarEqFGjSE9PB2w9R+PGjWPXrl2YTCZMJpM95puHBe7evZuHH34YT09PSpYsSb9+/UhKSrKv79WrFx07dmTSpEmUKVOGkiVLMmDAAPu5ciImJoYOHTrg4+ODn58fzzzzjMPzfbt27aJVq1b4+vri5+dHgwYN2L59OwDHjx+nffv2BAYG4u3tTa1atfj5559zHIvcg2L/gJltYMVoW2JVqSW8vBki/67ESiSLZm86SlxCKmUDPHn+gfJGhyMixVCRKmhRaFmtkH6l4M/r6gVZmPDQxcWFHj16MGfOHN566y37JIkLFy7EbDbTtWtXkpKSaNCgAcOHD8fPz4+ffvqJ7t27U7lyZRo3bnzXc1gsFjp37kxwcDC//fYb8fHxDs9nXefr68ucOXMIDQ1l9+7d9O3bF19fX15//XW6dOnCH3/8wdKlS1m50vZwvr+//y3HSE5OJioqiiZNmrBt2zbOnj1Lnz59GDhwoEMCuXr1asqUKcPq1as5dOgQXbp0oV69evTt2/eu7yez93c9sVq7di0ZGRkMGDCALl26sGbNGgC6detG/fr1mTZtGs7OzkRHR+PqarsrOmDAANLS0li3bh3e3t7s2bMHHx+fbMch96CMVFj/oe1lyQB3f4h6F+o/n6X//0XE5vKVNKatOQzAa49Wxd1FNyVEJO8pucoL6VfgvdCCP++bp8Eta+VjX3zxRSZOnMjatWtp2bIlYBsS+NRTT+Hv74+/vz/Dhg2zbz9o0CCWLVvGd999l6XkauXKlezbt49ly5YRGmr7LN57771bnpMaOXKk/efw8HCGDRvG/Pnzef311/H09MTHxwcXF5c7DgOcN28eKSkpfP3113h7297/lClTaN++Pe+//769dzMwMJApU6bg7OxM9erVefzxx1m1alWOkqtVq1axe/dujh49ai/T//XXX1OrVi22bdtGo0aNiImJ4R//+AfVq1cHICLirwpUMTExPPXUU9SuXRuwVb4UuauT2+HHgXBur225+hPw2CTwy7yoj4jc3r/WHCYxJYPqIb50qFfW6HBEpJgqUtUCJeeqV69O06ZNmTVrFgCHDh1i/fr19O7dGwCz2cw777xD7dq1KVGiBD4+PixbtoyYmJgsHX/v3r2EhYXZEysg0wqNCxYsoFmzZoSEhODj48PIkSOzfI4bz1W3bl17YgXQrFkzLBYL+/fvt7fVqlXLYbLpMmXKcPbs2Wyd68ZzhoWFOcx/VrNmTQICAti71/aH79ChQ+nTpw9t2rRhwoQJHD582L7tK6+8wj//+U+aNWvGmDFjclRARO4haVdg2Vsw8xFbYuVdGv42B7r8W4mVSA6cvnyVOZuOATC8bXWcndTrKyL5Qz1XecHVy9aLZMR5s6F3794MGjSIqVOnMnv2bCpXrkyLFi0AmDhxIp988gmTJ0+mdu3aeHt7M2TIENLS0vIs3M2bN9OtWzfGjRtHVFQU/v7+zJ8/nw8//DDPznGj60PyrjOZTFgslnw5F9gqHT733HP89NNP/PLLL4wZM4b58+fTqVMn+vTpQ1RUFD/99BPLly9n/PjxfPjhhwwaNCjf4pEi6uh6WDIILh21LdfpAm0ngFcJY+MSKcI+XnGAtAwLjSuWoGW1givGJCL3HvVc5QWTyTY8r6Bf2Xze4plnnsHJyYl58+bx9ddf8+KLL9qfv9q4cSMdOnTg+eefp27dulSqVIkDBw5k+dg1atTgxIkTnDlzxt62ZcsWh202bdpEhQoVeOutt2jYsCEREREcP37cYRs3NzfMZvNdz7Vr1y6Sk5PtbRs3bsTJyYlq1aplOebsuP7+Tpw4YW/bs2cPly9fpmbNmva2qlWr8uqrr7J8+XI6d+7M7Nmz7evCwsJ46aWX+OGHH3jttdeYMWNGvsQqRVRKPPxnsK3E+qWj4FcWnlsInb9QYiWSCwfjEvm/nScBeKNddft1T0QkPyi5uof4+PjQpUsXRowYwZkzZ+jVq5d9XUREBCtWrGDTpk3s3buXv//97w6V8O6mTZs2VK1alZ49e7Jr1y7Wr1/PW2+95bBNREQEMTExzJ8/n8OHD/Ppp5+yaNEih23Cw8Pt852dP3+e1NTUW87VrVs3PDw86NmzJ3/88QerV69m0KBBdO/ePdcl981ms33uteuvvXv30qZNG2rXrk23bt3YuXMnW7dupUePHrRo0YKGDRty9epVBg4cyJo1azh+/DgbN25k27Zt1KhRA4AhQ4awbNkyjh49ys6dO1m9erV9nQgHlsHUB2DHHNtywxfh5S1Q9VFDwxIpDj5Yth+LFaJqBXN/+UCjwxGRYk7J1T2md+/eXLp0iaioKIfno0aOHMn9999PVFQULVu2JCQkhI4dO2b5uE5OTixatIirV6/SuHFj+vTpw7vvvuuwzZNPPsmrr77KwIEDqVevHps2bWLUqFEO2zz11FO0bduWVq1aUbp06UzLwXt5ebFs2TIuXrxIo0aNePrpp2+ZTDqnkpKSqF+/vsOrffv2mEwmfvzxRwIDA2nevDlt2rShUqVKLFiwAABnZ2cuXLhAjx49qFq1Ks888wzt2rVj3LhxgC1pGzBgADVq1KBt27ZUrVqVf/3rX7mOV4q45Avwf31h3jOQeBoCK0LP/8ITH4OHn9HRiRR5249dZMWeOJxM8I+o/BnZICJyI5PVarUaHURhk5CQgL+/P/Hx8fj5Of6Bk5KSwtGjR6lYsSIeHh4GRSjFmX7H7gFWK/zxf/DL63DlApicoMkAaPkmuGXvWcri5k7fv/c6fTbZY7VaeebzzWw7dolnG4Ux4ak6RockIkVUdr5/VdBCRKQgJZyBn4bC/muTSAfVhCenQLkGxsYlUsys2nuWbccu4e7ixJA2VY0OR0TuEUquREQKgtUKv38Dy0ZCajw4ucJDr9leLm5GRydSrJgtVj5Ytg+AF5pVJMRfowBEpGAouRIRyW+XjsGSV+DoWtty6P3QYQoE1zI0LJHi6oedJzkQl4Sfhwv9W1Q2OhwRuYeooIWISH6xmGHLNPhXE1ti5eIBj7wDvVcosSoGpk6dSnh4OB4eHkRGRrJ169Y7bj958mSqVauGp6cnYWFhvPrqq6SkpBRQtPeOlHQzH6+wTSUyoFUV/L1c77KHiEjeUc+ViEh+OLcffhwIJ6/9wV2hGTz5GZTUXfTiYMGCBQwdOpTp06cTGRnJ5MmTiYqKYv/+/QQFBd2y/bx583jjjTeYNWsWTZs25cCBA/Tq1QuTycRHH31kwDsovr7ZfJzT8SmU8fegZ9Nwo8MRkXuMeq5ySEUWJb/od6uIM6fDuokw/UFbYuXmC49/ZCuxrsSq2Pjoo4/o27cvL7zwAjVr1mT69Ol4eXkxa9asTLfftGkTzZo147nnniM8PJxHH32Url273rW3S7In/mo6U9ccAuDVNlXxcHU2OCIRudcoucomV1fb8IIrV64YHIkUV2lpaYBt7iwpYs7sghmt4Nd/gjkNqjwCA7ZAo97gpK/b4iItLY0dO3bQpk0be5uTkxNt2rRh8+bNme7TtGlTduzYYU+mjhw5ws8//8xjjz122/OkpqaSkJDg8JI7+3ztYS5fSadKkA+d7y9rdDgicg/SsMBscnZ2JiAggLNnzwK2CW1NJpPBUUlxYbFYOHfuHF5eXri46H/PIiM9Bda+Dxs/AasZPAOh7ftQ5xnQ90Oxc/78ecxmM8HBwQ7twcHB7Nu3L9N9nnvuOc6fP8+DDz6I1WolIyODl156iTfffPO25xk/frx9InK5u7iEFGZtPArA61HVcHHWDQ0RKXj66y0HQkJCAOwJlkhecnJyonz58krai4qYLbZnqy4ctC3X7AiPTQSfW5+7kXvXmjVreO+99/jXv/5FZGQkhw4dYvDgwbzzzjuMGjUq031GjBjB0KFD7csJCQmEhYUVVMhFzuSVB0lJt9CgQiCP1Ay++w4iIvlAyVUOmEwmypQpQ1BQEOnp6UaHI8WMm5sbThpCVvilJsGqt2HrF4AVfILh8Q+hRnujI5N8VqpUKZydnYmLi3Noj4uLs998u9moUaPo3r07ffr0AaB27dokJyfTr18/3nrrrUz/n3d3d8fd3T3v30AxdPhcEt9tPwHA8LbVdXNKRAyj5CoXnJ2d9VyMyL3o8K+wZDDEx9iW6z0PUf+0DQeUYs/NzY0GDRqwatUqOnbsCNiG9K5atYqBAwdmus+VK1duSaCuXz9UxCb3Ji3bj9lipXX1IBpXLGF0OCJyD1NyJSKSVVcvwbKREP1v27J/eWg/Gaq0NjQsKXhDhw6lZ8+eNGzYkMaNGzN58mSSk5N54YUXAOjRowdly5Zl/PjxALRv356PPvqI+vXr24cFjho1ivbt2+smXS5Fn7jML3/EYjLBP9pWMzocEbnHGTr2aN26dbRv357Q0FBMJhOLFy+2r0tPT2f48OHUrl0bb29vQkND6dGjB6dPn77jMceOHYvJZHJ4Va9ePZ/fiYgUe3v/C1MjryVWJmj8d3h5sxKre1SXLl2YNGkSo0ePpl69ekRHR7N06VJ7kYuYmBjOnDlj337kyJG89tprjBw5kpo1a9K7d2+ioqL4/PPPjXoLxYLVamXCL3sB6Fy/HNVD/AyOSETudYb2XCUnJ1O3bl1efPFFOnfu7LDuypUr7Ny5k1GjRlG3bl0uXbrE4MGDefLJJ9m+ffsdj1urVi1WrlxpX1bVNRHJsaSz8PM/YM9i23LJCNtkwBWaGBqWGG/gwIG3HQa4Zs0ah2UXFxfGjBnDmDFjCiCye8faA+fYcuQibs5OvPpIhNHhiIgYm1y1a9eOdu3aZbrO39+fFStWOLRNmTKFxo0bExMTQ/ny5W97XBcXl9s+VCwikiVWK/zvO1g63DYc0OQMzV6BFm+Aq4fR0Ync8ywWK+8v3Q9AjyYVKBfoZXBEIiJF7Jmr+Ph4TCYTAQEBd9zu4MGDhIaG4uHhQZMmTRg/fvwdk7HU1FRSU1Pty5qoUeQeF38S/vsqHFxuWw6pDU9OgdB6hoYlIn9Zsus0e88k4OvuwoBWVYwOR0QEMPiZq+xISUlh+PDhdO3aFT+/24+pjoyMZM6cOSxdupRp06Zx9OhRHnroIRITE2+7z/jx4/H397e/NI+IyD0seh5MfcCWWDm7wcOjoO9qJVYihUhqhplJy229Vi+1rEygt5vBEYmI2BSJnqv09HSeeeYZrFYr06ZNu+O2Nw4zrFOnDpGRkVSoUIHvvvuO3r17Z7qPJmoUEcwZsGIUbPmXbblcY+gwBUqr+phIYTPvtxhOXrpKaV93XmgWbnQ4IiJ2hT65up5YHT9+nF9//fWOvVaZCQgIoGrVqhw6dOi222iiRpF73NXL8P2LcHiVbbnlCGj+D3BSiWyRwiYxJZ3PfrVd04e0icDLrdD/KSMi95BCPSzwemJ18OBBVq5cScmSJbN9jKSkJA4fPkyZMmXyIUIRKfIuHIYv29gSKxdP+NtX0PINJVYihdSM9Ue5mJxGxVLePNNQo0xEpHAxNLlKSkoiOjqa6OhoAI4ePUp0dDQxMTGkp6fz9NNPs337dubOnYvZbCY2NpbY2FjS0tLsx2jdujVTpkyxLw8bNoy1a9dy7NgxNm3aRKdOnXB2dqZr164F/fZEpLA7vBpmPAwXDoJfWei9DGp1NDoqEbmNc4mpfLn+CAD/iKqGq3OhvkcsIvcgQ/vSt2/fTqtWrezL15976tmzJ2PHjmXJkiUA1KtXz2G/1atX07JlSwAOHz7M+fPn7etOnjxJ165duXDhAqVLl+bBBx9ky5YtlC5dOn/fjIgULVtnwC/DwWqGco2gy1zwDTY6KhG5g89+PciVNDN1y/nT7j5NuSIihY+hyVXLli2xWq23XX+nddcdO3bMYXn+/Pm5DUtEijNzOvzyOmyfZVuu8yy0/0RzV4kUcscvJDPvtxgAhrerjslkMjgiEZFb6SlQEbl3XLkI3/WAY+sBE7QZC80Gg/5IEyn0Ji0/QIbFSvOqpWlauZTR4YiIZErJlYjcG87ug2+7wKVj4OYDT30J1drddTcRMd4fp+L5z67TAAxvq+kRRKTwUnIlIsXfgeW2UutpiRBQAbrOh+CaRkclIln0/tJ9AHSoF0qtUH+DoxERuT0lVyJSfFmtsHkKLB8FWKFCM3jmG/DO/rQOImKMjYfOs/7geVydTbz2iHqtRKRwU3IlIsVTRir891WInmtbvr8HPPYhuLgZG5eIZJnFYmXCL7Zeq26RFShf0svgiERE7kzJlYgUP0lnYcHzcOI3MDlB1HiI/LsKV4gUMT//cYbdp+LxdnNm4MNVjA5HROSulFyJSPESuxu+7QrxJ8DdH/42G6q0NjoqEcmmdLOFScv2A9C3eSVK+bgbHJGIyN0puRKR4mPvf+CHfpB+BUpWsRWuKBVhdFQikgPzt53g2IUrlPR2o89DlYwOR0QkS5RciUjRZ7XC+knw6z9ty5Va2XqsPAONjUtEciQ5NYNPVh4E4JXWEfi4688VESka9G0lIkVb+lX4cQD88X+25ciX4NF3wVlfbyJF1awNRzmflEr5El50bVze6HBERLJMf32ISNGVcBrmPwenfwcnF3hsEjR8weioRCQXLian8fm6IwC89mhV3FycDI5IRCTrlFyJSNF0agd8+xwkxYJnCXjma6j4kNFRiUguTfn1EEmpGdQK9aN9nVCjwxERyRYlVyJS9Oz+3jYUMCMFSteArt9CiYpGRyUiuXTi4hX+veU4AMPbVsfJSdMniEjRouRKRIoOiwXWvAfrJtqWq7aFzjPAw8/YuEQkT3y84gBpZgtNK5fkoYhSRocjIpJtSq5EpGhITYJFf4d9/7UtNxsMrceAk7OxcYlInth7JoFF0acAW6+VSZN+i0gRpORKRAq/yzG2iYHj/gBnN2j/KdTranRUIpKHPli6D6sVHq9dhrphAUaHIyKSI0quRKRwi9kC87vBlfPgHQTPzoWwxkZHJSJ56LcjF1i9/xzOTiaGRVUzOhwRkRxTciUihdfvc+E/g8GSDiG14dlvISDM6KhEJA9ZrVYmLN0HwLONwqhYytvgiEREck6TR4hI4WMxw7K34MeXbYlVjSfhxWVKrCRXwsPDefvtt4mJiTE6FLnBsj/j+D3mMp6uzgxuHWF0OCIiuaLkSkQKl5R4+PZZ2DzFttxiOPztK3DT3WzJnSFDhvDDDz9QqVIlHnnkEebPn09qaqrRYd3TMswWJi6z9Vr1frAiQX4eBkckIpI7Sq5EpPC4eAS+fAQOLgcXD3h6FrR6E5z0VSW5N2TIEKKjo9m6dSs1atRg0KBBlClThoEDB7Jz506jw7snfb/jJIfPJRPo5Uq/FpWMDkdEJNcM/Ytl3bp1tG/fntDQUEwmE4sXL3ZYb7VaGT16NGXKlMHT05M2bdpw8ODBux536tSphIeH4+HhQWRkJFu3bs2ndyAieeboOpjxMJzfD75l4IVf4L6njI5KiqH777+fTz/9lNOnTzNmzBi+/PJLGjVqRL169Zg1axZWq9XoEO8JV9PMTF5pu6YPaFUFPw9XgyMSEck9Q5Or5ORk6taty9SpUzNd/8EHH/Dpp58yffp0fvvtN7y9vYmKiiIlJeW2x1ywYAFDhw5lzJgx7Ny5k7p16xIVFcXZs2fz622ISG5tmwnfdIKrl6BsA+i7Gsreb3RUUkylp6fz3Xff8eSTT/Laa6/RsGFDvvzyS5566inefPNNunXrlqXjZPdG3uXLlxkwYABlypTB3d2dqlWr8vPPP+fFWyqS5mw6RmxCCmUDPHn+gQpGhyMikicMrRbYrl072rVrl+k6q9XK5MmTGTlyJB06dADg66+/Jjg4mMWLF/Pss89mut9HH31E3759eeGFFwCYPn06P/30E7NmzeKNN97InzciIjljToelI2DbDNty7WfgyU/B1dPYuKRY2rlzJ7Nnz+bbb7/FycmJHj168PHHH1O9enX7Np06daJRo0Z3Pdb1G3nTp08nMjKSyZMnExUVxf79+wkKCrpl+7S0NB555BGCgoL4/vvvKVu2LMePHycgICAv32KRcflKGtPWHAJg6CNV8XDVZOBFgdlsJj093egwRPKcq6srzs558z1UaEuxHz16lNjYWNq0aWNv8/f3JzIyks2bN2eaXKWlpbFjxw5GjBhhb3NycqJNmzZs3rz5tudKTU11eKg5ISEhj96FiNzWlYuwsBccXWtbbj0aHhwKJpOhYUnx1ahRIx555BGmTZtGx44dcXW9dRhaxYoVb3vz7kbZvZE3a9YsLl68yKZNm+znDQ8Pz90bKsKmrTlMQkoG1YJ96Vi/rNHhyF1YrVZiY2O5fPmy0aGI5JuAgABCQkIw5fLvkEKbXMXGxgIQHBzs0B4cHGxfd7Pz589jNpsz3Wffvn23Pdf48eMZN25cLiMWkSw7dwC+7WIrYOHqDU/NgOqPGx2VFHNHjhyhQoU7Dz/z9vZm9uzZd9wmJzfylixZQpMmTRgwYAA//vgjpUuX5rnnnmP48OG3vVtaXG/8nb58ldmbjgEwvF01nJ10Q6Wwu55YBQUF4eXlles/PkUKE6vVypUrV+yPEJUpUyZXxyu0yVVBGjFiBEOHDrUvJyQkEBam+XRE8sXBlfD9i5AaD/7loeu3EHKf0VHJPeDs2bPExsYSGRnp0P7bb7/h7OxMw4YNs3ScnNzIO3LkCL/++ivdunXj559/5tChQ7z88sukp6czZsyYTPcprjf+Jq88QFqGhcbhJWhV7dYhlFK4mM1me2JVsmRJo8MRyReenrbHEc6ePUtQUFCuhggW2vrGISEhAMTFxTm0x8XF2dfdrFSpUjg7O2drHwB3d3f8/PwcXiKSx6xW2PwvmPc3W2JVvgn0/VWJlRSYAQMGcOLEiVvaT506xYABA/L13BaLhaCgIL744gsaNGhAly5deOutt5g+ffpt9xkxYgTx8fH2V2axFzUH4xL5fsdJAIa3q64ekCLg+jNWXl5eBkcikr+u/47n9rnCQptcVaxYkZCQEFatWmVvS0hI4LfffqNJkyaZ7uPm5kaDBg0c9rFYLKxateq2+4hIAchIgyWDYNkIsFqg/vPQYwn4lDY6MrmH7Nmzh/vvv7UKZf369dmzZ0+Wj5OTG3llypShatWqDndDa9SoQWxsLGlpaZnuUxxv/H2wbD8WKzxaM5gGFQKNDkeyQYmwFHd59TtuaHKVlJREdHQ00dHRgK2IRXR0NDExMZhMJoYMGcI///lPlixZwu7du+nRowehoaF07NjRfozWrVszZcoU+/LQoUOZMWMGX331FXv37qV///4kJyfbHzoWkQKWfB6+7gC/fwMmJ4gaD09OARc3oyOTe4y7u/stCRHAmTNncHHJ+ij5nNzIa9asGYcOHcJisdjbDhw4QJkyZXBzuzf+X9hx/CIr9sThZILX21YzOhwRkXxhaHK1fft26tevT/369QFbYlS/fn1Gjx4NwOuvv86gQYPo168fjRo1IikpiaVLl+Lh4WE/xuHDhzl//rx9uUuXLkyaNInRo0dTr149oqOjWbp06S1j40WkAMT9CV+0gphN4O4Hzy2EJi+rIqAY4tFHH7UPtbvu8uXLvPnmmzzyyCPZOtbdbuT16NHDoeBF//79uXjxIoMHD+bAgQP89NNPvPfee/k+HLGwsFqtTPjF9jza3xqEUSXI1+CIRHImPDycyZMnZ3n7NWvWYDKZVGnxHmKyair6WyQkJODv7098fHyxGIYhYoh9P8MPfSEtCUpUgq7zobTuVsud5ef376lTp2jevDkXLlyw39SLjo4mODiYFStWZLuQ0ZQpU5g4cSKxsbHUq1ePTz/91F4so2XLloSHhzNnzhz79ps3b+bVV18lOjqasmXL0rt37ztWC7xZUb42rdobR++vtuPu4sSaf7SkjL/msisqUlJSOHr0KBUrVnS4uV3Y3W2I15gxYxg7dmy2j3vu3Dm8vb2z/AxaWloaFy9eJDg4uMCGVlavXp2jR49y/PjxO9YcEEd3+l3PzvevkqtMFOULmIjhrFbY8BGsegewQsUW8Lc54FXC6MikCMjv79/k5GTmzp3Lrl278PT0pE6dOnTt2jXTOa8Km6J6bTJbrDz2yXr2xyXy9xaVGNGuhtEhSTYU1eTqxml7FixYwOjRo9m/f7+9zcfHBx8fH8DWs2o2m7M1PLiw2rBhA926dePBBx+kTp06DB8+3NB40tPTi8T3K+RdclVoC1qISBGUngI/9INVbwNWaNQXnv8/JVZSaHh7e9OvXz+mTp3KpEmT6NGjR5G58BdVi34/xf64RPw8XHi5RRWjw5F7REhIiP3l7++PyWSyL+/btw9fX19++eUXGjRogLu7Oxs2bODw4cN06NCB4OBgfHx8aNSoEStXrnQ47s3DAk0mE19++SWdOnXCy8uLiIgIlixZYl9/87DAOXPmEBAQwLJly6hRowY+Pj60bduWM2fO2PfJyMjglVdeISAggJIlSzJ8+HB69uzpUHPgdmbOnMlzzz1H9+7dmTVr1i3rT548SdeuXSlRogTe3t40bNiQ3377zb7+P//5D40aNcLDw4NSpUrRqVMnh/e6ePFih+MFBATYe+iPHTuGyWRiwYIFtGjRAg8PD+bOncuFCxfo2rUrZcuWxcvLi9q1a/Ptt986HMdisfDBBx9QpUoV3N3dKV++PO+++y4ADz/8MAMHDnTY/ty5c7i5uTk8+1pYFP0UXUQKh8RYmP8cnNoBJmd47ANo1MfoqERusWfPHmJiYm6p0vfkk08aFFHxlZJu5uMVBwB4uVUV/L2UyBYHVquVq+lmQ87t6eqcZ8Pr3njjDSZNmkSlSpUIDAzkxIkTPPbYY7z77ru4u7vz9ddf0759e/bv30/58uVve5xx48bxwQcfMHHiRD777DO6devG8ePHKVEi8xuLV65cYdKkSXzzzTc4OTnx/PPPM2zYMObOnQvA+++/z9y5c5k9ezY1atTgk08+YfHixbRq1eqO7ycxMZGFCxfy22+/Ub16deLj41m/fj0PPfQQYCsk16JFC8qWLcuSJUsICQlh586d9kI7P/30E506deKtt97i66+/Ji0tjZ9//jlHn+uHH35I/fr18fDwICUlhQYNGjB8+HD8/Pz46aef6N69O5UrV6Zx48aAbeqJGTNm8PHHH/Pggw9y5swZ+7yBffr0YeDAgXz44Ye4u7sD8O9//5uyZcvy8MMPZzu+/KbkSkRy7/Tv8O1zkHgaPALgma+hUgujoxJxcOTIETp16sTu3bsxmUxcHxV//Q81s9mYPxaLs39vOc6py1cJ8fOgV9Nwo8ORPHI13UzN0csMOfeet6PwcsubP1/ffvtth2I2JUqUoG7duvbld955h0WLFrFkyZJbek5u1KtXL7p27QrAe++9x6effsrWrVtp27Ztptunp6czffp0KleuDMDAgQN5++237es/++wzRowYYe81mjJlSpaSnPnz5xMREUGtWrUAePbZZ5k5c6Y9uZo3bx7nzp1j27Zt9sSvSpW/epPfffddnn32WYfJy2/8PLJqyJAhdO7c2aFt2LBh9p8HDRrEsmXL+O6772jcuDGJiYl88sknTJkyhZ49ewJQuXJlHnzwQQA6d+7MwIED+fHHH3nmmWcAWw9gr169CuUUATkaFnjixAlOnjxpX966dStDhgzhiy++yLPARKSI+HMRzGpnS6xKVbNNDKzESgqhwYMHU7FiRc6ePYuXlxd//vkn69ato2HDhqxZs8bo8IqdhJR0pqw+BMCrj0Tg4Zq1wh0iBaVhw4YOy0lJSQwbNowaNWoQEBCAj48Pe/fuJSYm5o7HqVOnjv1nb29v/Pz8OHv27G239/LysidWYJsH7/r28fHxxMXF2Xt0AJydnWnQoMFd38+sWbN4/vnn7cvPP/88CxcuJDExEbAV8Klfv/5te9Sio6Np3br1Xc9zNzd/rmazmXfeeYfatWtTokQJfHx8WLZsmf1z3bt3L6mpqbc9t4eHh8Mwx507d/LHH3/Qq1evXMeaH3KU+j/33HP069eP7t27ExsbyyOPPEKtWrWYO3cusbGx9lLqIlKMWSywdgKsfd+2XOUReHomePgbG5fIbWzevJlff/2VUqVK4eTkhJOTEw8++CDjx4/nlVde4ffffzc6xGLl87WHuXwlncqlvXnq/nJGhyN5yNPVmT1vRxl27rzi7e3tsDxs2DBWrFjBpEmTqFKlCp6enjz99NO3nej7upuf2zSZTA5z2mVl+9zWl9uzZw9btmxh69atDkUszGYz8+fPp2/fvnh63rlK593WZxZnenr6Ldvd/LlOnDiRTz75hMmTJ1O7dm28vb0ZMmSI/XO923nBNjSwXr16nDx5ktmzZ/Pwww9ToUKFu+5nhBz1XP3xxx/2jPq7777jvvvuY9OmTcydO9eh7KyIFFNpybCw51+JVZOB8NwCJVZSqJnNZnx9bfMrlSpVitOnTwNQoUIFhypikntnE1KYueEoAK+3rY6Ls+pnFScmkwkvNxdDXvk5DGzjxo306tWLTp06Ubt2bUJCQjh27Fi+nS8z/v7+BAcHs23bNnub2Wxm586dd9xv5syZNG/enF27dhEdHW1/DR06lJkzZwK2Hrbo6GguXryY6THq1KlzxwIRpUuXdii8cfDgQa5cuXLX97Rx40Y6dOjA888/T926dalUqRIHDhywr4+IiMDT0/OO565duzYNGzZkxowZzJs3jxdffPGu5zVKjnqu0tPT7Q+UrVy50v4QcPXq1R0+dBEphuJPwrfPQuxucHKF9pOh/vN33U3EaPfddx+7du2iYsWKREZG8sEHH+Dm5sYXX3xBpUqVjA6vWJm86iAp6RbuLx/AozWDjQ5HJEsiIiL44YcfaN++PSaTiVGjRt2xByq/DBo0iPHjx1OlShWqV6/OZ599xqVLl26bWKanp/PNN9/w9ttvc9999zms69OnDx999BF//vknXbt25b333qNjx46MHz+eMmXK8PvvvxMaGkqTJk0YM2YMrVu3pnLlyjz77LNkZGTw888/23vCHn74YaZMmUKTJk0wm80MHz48S9VWIyIi+P7779m0aROBgYF89NFHxMXFUbNmTcA27G/48OG8/vrruLm50axZM86dO8eff/5J7969Hd7LwIED8fb2dqhiWNjk6FZSrVq1mD59OuvXr2fFihX2B/ZOnz5NyZIl8zRAESlETmyFL1rZEiuvUtDrv0qspMgYOXKk/Q+lt99+m6NHj/LQQw/x888/8+mnnxocXfFx5FwSC7adAGB42+qF8oFzkcx89NFHBAYG0rRpU9q3b09UVBT3339/gccxfPhwunbtSo8ePWjSpAk+Pj5ERUXddp6xJUuWcOHChUwTjho1alCjRg1mzpyJm5sby5cvJygoiMcee4zatWszYcIE+0TmLVu2ZOHChSxZsoR69erx8MMPs3XrVvuxPvzwQ8LCwnjooYd47rnnGDZsWJYmUx45ciT3338/UVFRtGzZkpCQkFvKyo8aNYrXXnuN0aNHU6NGDbp06XLLc2tdu3bFxcWFrl27Fuo513I0ifCaNWvo1KkTCQkJ9OzZ0/6A2Ztvvsm+ffv44Ycf8jzQglRUJ2oUyVe75sOSQWBOg+D7oOu3EHD70rQiOVHQ378XL14kMDCwSCQAReXa9PLcHfy8O5aHqwcxq1cjo8ORXCqqkwgXJxaLhRo1avDMM8/wzjvvGB2OYY4dO0blypXZtm1bviS9eTWJcI6GBbZs2ZLz58+TkJBAYGCgvb1fv35ZymBFpAixmG2TAm+cbFuu/gR0+hzcfQwNSyQ70tPT8fT0JDo62mHYzO2qZknORJ+4zM+7YzGZ4PW21YwOR6RIOn78OMuXL6dFixakpqYyZcoUjh49ynPPPWd0aIZIT0/nwoULjBw5kgceeMCQ3sTsyNGwwKtXr5KammpPrI4fP87kyZPZv38/QUFBeRqgiBgoJcE2MfD1xKr5P+CZb5RYSZHj6upK+fLlNZdVPrJarbz/i23Sz071y1I9pPD2rokUZk5OTsyZM4dGjRrRrFkzdu/ezcqVK6lRo4bRoRli48aNlClThm3btjF9+nSjw7mrHPVcdejQgc6dO/PSSy9x+fJlIiMjcXV15fz583z00Uf0798/r+MUkYJ28Sh82xXO7QUXD+gwFWo/bXRUIjn21ltv8eabb/LNN9+oxyofrDt4ns1HLuDm7MTQR6oaHY5IkRUWFsbGjRuNDqPQaNmyZa5L1RekHCVXO3fu5OOPPwbg+++/Jzg4mN9//53/+7//Y/To0UquRIq6YxtgQXe4ehF8QqDrPCh79wkMRQqzKVOmcOjQIUJDQ6lQocItc7HcrdSx3J7FYmXCtV6r7k0qUC5QjwiIyL0pR8nVlStX7HOFLF++nM6dO+Pk5MQDDzzA8ePH8zRAESlgO+bAT6+BJQNC68Oz88Av1OioRHLt5upUknf+87/T7D2TgK+7CwNaVTE6HBERw+QouapSpQqLFy+mU6dOLFu2jFdffRWAs2fPFuoKRiJyB+YMWP4W/HZtPHOtzrahgG66Ay3Fw5gxY4wOoVhKy7AwabltEua/t6hECW83gyMSETFOjgpajB49mmHDhhEeHk7jxo1p0qQJYOvFql+/fp4GKCIF4OolmPv0X4lVq5Hw9CwlViJyV/N+O86Ji1cp7evOiw9WNDocERFD5ajn6umnn+bBBx/kzJkz1K1b197eunXrQj1jsohk4vwh+LYLXDgErl7Q+Quo0d7oqETynJOT0x3ns1IlwexLSs3gs18PATC4dQRebjn6s0JEpNjI8bdgSEgIISEhnDx5EoBy5crRuHHjPAtMRArA4V9hYS9IiQe/craJgcvUMToqkXyxaNEih+X09HR+//13vvrqK8aNG2dQVEXbjHVHuJCcRsVS3nRpFGZ0OCIihsvRsECLxcLbb7+Nv78/FSpUoEKFCgQEBPDOO+9gsVjyOkYRyWtWK/z2Ofz7aVtiFRYJ/VYrsZJirUOHDg6vp59+mnfffZcPPviAJUuWGB1ekXMuMZUv1x8BYNij1XB1ztGfFCKFWsuWLRkyZIh9OTw8nMmTJ99xH5PJxOLFi3N97rw6jhSsHPVcvfXWW8ycOZMJEybQrFkzADZs2MDYsWNJSUnh3XffzdMgRSQPZaTBL/+wVQUEqPsctJ8MLu5GRiVimAceeIB+/foZHUaRM+XXgySnmalTzp/HaocYHY6Ig/bt25Oens7SpUtvWbd+/XqaN2/Orl27qFMnezcVt23bdss0Drk1duxYFi9eTHR0tEP7mTNnCAwMzNNz3c7Vq1cpW7YsTk5OnDp1Cnd3/U2QUzm6zfTVV1/x5Zdf0r9/f+rUqUOdOnV4+eWXmTFjBnPmzMnTAMPDwzGZTLe8BgwYkOn2c+bMuWVbDw+PPI1JpMhKvgDfdLqWWJng0X9Cx38psZJ71tWrV/n0008pW7as0aEUKccvJDNvawwAb7Stfsdn2USM0Lt3b1asWGF/fOVGs2fPpmHDhtlOrABKly6Nl1fBFHsKCQkpsCTn//7v/6hVqxbVq1c3vLfMarWSkZFhaAy5kaPk6uLFi1SvXv2W9urVq3Px4sVcB3Wjbdu2cebMGftrxYoVAPztb3+77T5+fn4O+2juLRHg7F6Y0QqObwA3X3juO2g6CPRHkdwjAgMDKVGihP0VGBiIr68vs2bNYuLEiUaHV6R8uPwA6WYrD0WUommVUkaHI3KLJ554gtKlS99y0z8pKYmFCxfSu3dvLly4QNeuXSlbtixeXl7Url2bb7/99o7HvXlY4MGDB2nevDkeHh7UrFnT/nfqjYYPH07VqlXx8vKiUqVKjBo1ivT0dMDWKTBu3Dh27dpl7xS4HvPNwwJ3797Nww8/jKenJyVLlqRfv34kJSXZ1/fq1YuOHTsyadIkypQpQ8mSJRkwYID9XHcyc+ZMnn/+eZ5//nlmzpx5y/o///yTJ554Aj8/P3x9fXnooYc4fPiwff2sWbOoVasW7u7ulClThoEDBwJw7NgxTCaTQ6/c5cuXMZlMrFmzBoA1a9ZgMpn45ZdfaNCgAe7u7mzYsIHDhw/ToUMHgoOD8fHxoVGjRqxcudIhrtTUVIYPH05YWBju7u5UqVKFmTNnYrVaqVKlCpMmTXLYPjo6GpPJxKFDh+76meRUjoYF1q1blylTpvDpp586tE+ZMiVHdwHupHTp0g7LEyZMoHLlyrRo0eK2+5hMJkJCNERBxG7/Uvi/3pCWBIHh0HUBBN16g0SkOPv4448delicnJwoXbo0kZGRBTb0pjj441Q8S3adBmB4W32P3JOsVki/Ysy5Xb2ydFPQxcWFHj16MGfOHN566y37//sLFy7EbDbTtWtXkpKSaNCgAcOHD8fPz4+ffvqJ7t27U7ly5SwVabNYLHTu3Jng4GB+++034uPjHZ7Pus7X15c5c+YQGhrK7t276du3L76+vrz++ut06dKFP/74g6VLl9oTB39//1uOkZycTFRUFE2aNGHbtm2cPXuWPn36MHDgQIcEcvXq1ZQpU4bVq1dz6NAhunTpQr169ejbt+9t38fhw4fZvHkzP/zwA1arlVdffZXjx49ToUIFAE6dOkXz5s1p2bIlv/76K35+fmzcuNHeuzRt2jSGDh3KhAkTaNeuHfHx8WzcuPGun9/N3njjDSZNmkSlSpUIDAzkxIkTPPbYY7z77ru4u7vz9ddf0759e/bv30/58uUB6NGjB5s3b+bTTz+lbt26HD16lPPnz2MymXjxxReZPXs2w4YNs59j9uzZNG/enCpV8m+y8xwlVx988AGPP/44K1eutM9xtXnzZk6cOMHPP/+cpwHeKC0tjX//+98MHTr0jkMQkpKSqFChAhaLhfvvv5/33nuPWrVq3Xb71NRUUlNT7csJCQl5GreIYaxW2PQprBgDWCH8IXjma/AqYXRkIgWuV69eRodQLLy/dB8AT9YN5b6yt/4RKPeA9CvwXqgx537zNLhl7ZmnF198kYkTJ7J27VpatmwJ2P64fuqpp/D398ff39/hD+9BgwaxbNkyvvvuuywlVytXrmTfvn0sW7aM0FDb5/Hee+/Rrl07h+1Gjhxp/zk8PJxhw4Yxf/58Xn/9dTw9PfHx8cHFxeWOHQPz5s0jJSWFr7/+2v7M15QpU2jfvj3vv/8+wcHBgK2HfsqUKTg7O1O9enUef/xxVq1adcfkatasWbRr185+kykqKorZs2czduxYAKZOnYq/vz/z58/H1dUVgKpVq9r3/+c//8lrr73G4MGD7W2NGjW66+d3s7fffptHHnnEvlyiRAmHKZ/eeecdFi1axJIlSxg4cCAHDhzgu+++Y8WKFbRp0waASpUq2bfv1asXo0ePZuvWrTRu3Jj09HTmzZt3S29WXsvRsMAWLVpw4MABOnXqxOXLl7l8+TKdO3fmzz//5JtvvsnrGO0WL17M5cuX73iBrFatGrNmzeLHH3/k3//+NxaLhaZNm2Y65va68ePH2/8n8/f3JyxM5WSlGEhPgcX9YcVowAoNXoDui5RYyT1r9uzZLFy48Jb2hQsX8tVXXxkQUdGz8dB51h88j6uziWGPVjM6HJE7ql69Ok2bNmXWrFkAHDp0iPXr19O7d2/ANrfdO++8Q+3atSlRogQ+Pj4sW7aMmJiYLB1/7969hIWF2RMrwN7pcKMFCxbQrFkzQkJC8PHxYeTIkVk+x43nqlu3rkMxjWbNmmGxWNi/f7+9rVatWjg7O9uXy5Qpw9mzZ297XLPZzFdffcXzzz9vb3v++eeZM2eOvQJ4dHQ0Dz30kD2xutHZs2c5ffo0rVu3ztb7yUzDhg0dlpOSkhg2bBg1atQgICAAHx8f9u7da//soqOjcXZ2vu1ottDQUB5//HH7f////Oc/pKam3vHRoryQ43muQkNDb6kKuGvXLmbOnMkXX3yR68AyM3PmTNq1a+fwS3yzJk2aOPxiN23alBo1avD555/zzjvvZLrPiBEjGDp0qH05ISFBCZYUbYlxsOB5OLkVTM7Q7n1o1EfPV8k9bfz48Xz++ee3tAcFBdGvXz969uxpQFRFh9VqtfdaPde4POVLFsxD/VIIuXrZepCMOnc29O7dm0GDBjF16lRmz57t8GjJxIkT+eSTT5g8eTK1a9fG29ubIUOGkJaWlmfhbt68mW7dujFu3DiioqLsPUAffvhhnp3jRjcnQCaT6Y7TJC1btoxTp07RpUsXh3az2cyqVat45JFH8PT0vO3+d1oHtuHXYPv+uO52z4DdXIVx2LBhrFixgkmTJlGlShU8PT15+umn7f997nZugD59+tC9e3c+/vhjZs+eTZcuXfK9IEmRmUr9+PHjrFy5kh9++CFb+7m6ulK/fv07Prjm7u6ukpNSfJzaCQu6Q8JJ8PCHv30FlVsZHZWI4WJiYqhYseIt7RUqVMj2XeR70c+7Y/nfyXi83ZwZ1DrC6HDESCZTlofmGe2ZZ55h8ODBzJs3j6+//pr+/fvbHy3ZuHEjHTp0sPfaWCwWDhw4QM2aNbN07Bo1anDixAnOnDlDmTJlANiyZYvDNps2baJChQq89dZb9rabC625ublhNpvveq45c+aQnJxsT0I2btyIk5MT1arlvBd55syZPPvssw7xAbz77rvMnDmTRx55hDp16vDVV1+Rnp5+S/Lm6+tLeHg4q1atolWrW//WuF474cyZM9SvXx/glpLzt7Nx40Z69epFp06dAFtP1rFjx+zra9eujcViYe3atfZhgTd77LHH8Pb2Ztq0aSxdupR169Zl6dy5UWRm/Js9ezZBQUE8/vjj2drPbDaze/du+y+9SLEVfxIWvwwzHrYlViUjoO9qJVYi1wQFBfG///3vlvZdu3ZRsmRJAyIqOtLNFiYus/Va9XmoEqV8dENSigYfHx+6dOnCiBEjOHPmjMOjJREREaxYsYJNmzaxd+9e/v73vxMXF5flY7dp04aqVavSs2dPdu3axfr1629JUiIiIoiJiWH+/PkcPnyYTz/9lEWLFjlsEx4eztGjR4mOjub8+fMOdQCu69atGx4eHvTs2ZM//viD1atXM2jQILp3725/3iq7zp07x3/+8x969uzJfffd5/Dq0aMHixcv5uLFiwwcOJCEhASeffZZtm/fzsGDB/nmm2/swxHHjh3Lhx9+yKeffsrBgwfZuXMnn332GWDrXXrggQeYMGECe/fuZe3atQ7PoN1JREQEP/zwA9HR0ezatYvnnnvOoRcuPDycnj178uKLL7J48WKOHj3KmjVr+O677+zbODs706tXL0aMGEFERESmwzbzWpFIriwWC7Nnz6Znz564uDh2tvXo0YMRI0bYl99++22WL1/OkSNH2LlzJ88//zzHjx+nT58+BR22SMG4ehlWjoXPGkD0XMAK9z0FfVZCycoGBydSeHTt2pVXXnmF1atXYzabMZvN/PrrrwwePJhnn33W6PAKtQXbTnDswhVKervRt3mlu+8gUoj07t2bS5cuERUV5fBoyciRI7n//vuJioqiZcuWhISE0LFjxywf18nJiUWLFnH16lUaN25Mnz59bnlk5sknn+TVV19l4MCB1KtXj02bNjFq1CiHbZ566inatm1Lq1atKF26dKbl4L28vFi2bBkXL16kUaNGPP3007Ru3ZopU6Zk78O4wfXiGJk9L9W6dWs8PT3597//TcmSJfn1119JSkqiRYsWNGjQgBkzZth7sXr27MnkyZP517/+Ra1atXjiiSc4ePCg/VizZs0iIyODBg0aMGTIEP75z39mKb6PPvqIwMBAmjZtSvv27YmKiuL+++932GbatGk8/fTTvPzyy1SvXp2+ffuSnJzssE3v3r1JS0vjhRdeyO5HlCMm642DIO+ic+fOd1x/+fJl1q5de9euzexavnw5UVFR7N+/36E6CUDLli0JDw+3l6F89dVX+eGHH4iNjSUwMJAGDRrwz3/+094VmRUJCQn4+/sTHx+Pn59fXr4VkbyTkQrbZsK6D+DqJVtbhQfhkbehXANjYxPJofz8/k1LS6N79+4sXLjQfqPOYrHQo0cPpk+fjpubW56eL68ZdW26kpZBi4lrOJeYytj2NenV7NahlVJ8paSkcPToUSpWrIiHh4fR4Yhk2/r162ndujUnTpy4Yy/fnX7Xs/P9m63kKqsZ3+zZs7N6yEJJyZUUahYL/PkDrHobLl8bt126OrQZB1WjVLRCirSC+P49ePAg0dHReHp6Urt2bftcLoWdUdemKb8eZNLyA4SV8GTV0Ja4uRSJQS+SR5RcSVGVmprKuXPn6NmzJyEhIcydO/eO2+dVcpWtghZFPWkSKfKOrrOVVj/9u23ZJwRavQn1uoFzkalPI2KoiIgIIiJyX5Bh6tSpTJw4kdjYWOrWrctnn32Wpblx5s+fT9euXenQoQOLFy/OdRz56WJyGp+vPQLAsEerKbESkSLj22+/pXfv3tSrV4+vv/66wM6rb0mRoiBuD8z9G3zV3pZYufnCwyPhlZ3QoKcSK5EseOqpp3j//fdvaf/ggw+yPe/JggULGDp0KGPGjGHnzp3UrVuXqKioO84nA3Ds2DGGDRvGQw89lK3zGWXq6kMkpmZQs4wf7esYNGmsiEgO9OrVC7PZzI4dOyhbtmyBnVfJlUhhFn8KfhwA05vBweXg5AKN/w6Do6H5P4pMKVyRwmDdunU89thjt7S3a9cu2+V5P/roI/r27csLL7xAzZo1mT59Ol5eXvbJKjNjNpvt891UqlT4i0KcvHSFbzbbhh4Pb1cdJycNORYRuRvd7hYpjFLiYeMnsPlfkHHV1lazI7QerQqAIjmUlJSUadEKV1dXEhISsnyctLQ0duzY4VCp1snJiTZt2rB58+bb7vf2228TFBRE7969Wb9+/V3Pk5qa6lCSOTsx5oWPVhwgzWyhSaWSNI8oVaDnlsInG4/oixRJefU7rp4rkcIkIw22TIdP6sH6D22JVfmm0HslPPOVEiuRXKhduzYLFiy4pX3+/PlZnjQU4Pz585jN5luqTgUHBxMbG5vpPhs2bGDmzJnMmDEjy+cZP348/v7+9ldYWFiW982tfbEJLPr9FABvtKtun3RV7j3Xy21fuXLF4EhE8tf13/GbJ0rOLvVciRQGViv8uQhWjYNLx2xtpapBm7FQrZ0qAIrkgVGjRtG5c2cOHz7Mww8/DMCqVauYN28e33//fb6dNzExke7duzNjxgxKlcp6D9CIESMYOnSofTkhIaHAEqwPlu7HaoXHaodQNyygQM4phZOzszMBAQH25wm9vLyUbEuxYrVauXLlCmfPniUgIABnZ+dcHU/JlYjRjm2A5aPg9E7bsk/wtQqAz6tQhUgeat++PYsXL+a9997j+++/x9PTk7p16/Lrr79SokSJLB+nVKlSODs7ExcX59AeFxdHSEjILdsfPnyYY8eO0b59e3ubxWIBwMXFhf3791O58q290u7u7ri7u2c5rrzy25EL/LrvLM5OJoY9Wq3Azy+Fz/Xf67sVbBEpygICAjL9Ds8u/eUmYpSze2HlWDiw1Lbs5gPNBkOTASpUIZJPHn/8cR5//HHA1hP07bffMmzYMHbs2IHZbM7SMdzc3GjQoAGrVq2iY8eOgC1ZWrVqFQMHDrxl++rVq7N7926HtpEjR5KYmMgnn3xSoMP97sZqtTJh6T4AujQKo1JpH4MjksLAZDJRpkwZgoKCSE9PNzockTzn6uqa6x6r65RciRS0hDOw5j34/d9gtdgqADboBS2Gg0+Q0dGJFHvr1q1j5syZ/N///R+hoaF07tyZqVOnZusYQ4cOpWfPnjRs2JDGjRszefJkkpOTeeGFFwDo0aMHZcuWZfz48Xh4eHDfffc57B8QEABwS7vRlu+J4/eYy3i6OjOkde7nApPixdnZOc/+ABUprpRciRSUlIRrFQCn/lUBsMaT0HoMlKpibGwixVxsbCxz5sxh5syZJCQk8Mwzz5CamsrixYuzVcziui5dunDu3DlGjx5NbGws9erVY+nSpfYiFzExMTg5Fa2aURlmCx9c67V68cFwgvw8DI5IRKToMVlVW/MWCQkJ+Pv7Ex8fj5+fn9HhSFGXkQY75sDaCXDlgq0t7AF49B0Ia2xoaCKFTX58/7Zv355169bx+OOP061bN9q2bYuzszOurq7s2rUrR8mVEfL72rRgWwzD/283AV6urHu9FX4euauYJSJSXGTn+1c9VyL5xWqFPT/aKgBePGJrKxlhqwBY/XFVABQpIL/88guvvPIK/fv3JyJCQ90yk5Ju5uMVBwEY2KqKEisRkRwqWmMWRIqK45vgyzawsKctsfIOgic+hpe3QI0nlFiJFKANGzaQmJhIgwYNiIyMZMqUKZw/f97osAqVOZuOEZuQQtkAT55/oILR4YiIFFlKrkTy0rn98G1XmN0OTm0HV29oOQJe+R0avqjS6iIGeOCBB5gxYwZnzpzh73//O/Pnzyc0NBSLxcKKFStITEw0OkRDxV9J51+rDwHw6iNV8XBVwQIRkZxSciWSFxJj4T+D4V8PwP6fweQMDXvbkqqWb4C7yhmLGM3b25sXX3yRDRs2sHv3bl577TUmTJhAUFAQTz75pNHhGeZfaw+RkJJBtWBfOtUva3Q4IiJFmpIrkdxITYTV78Gn9W1FK6wWqP4EDPgNnvgIfIONjlBEMlGtWjU++OADTp48ybfffmt0OIY5E3+VORuPAfB622o4O2nIsohIbmiMkkhOmNOvVQB8H5LP2drKNbZVACz/gKGhiUjWOTs707FjR/tkwPeaySsOkpphoVF4IA9X1zx7IiK5peRKJDusVti7BFaOg4uHbW0lq9jmqqrRXoUqRKTIOHQ2kYU7TgDwRrvqmPT9JSKSa0quRLIqZgssHwUnt9qWvUvbnqe6vyc4q2yxiBQtHyzdj8UKj9QMpkGFEkaHIyJSLCi5Ermbcwdsc1Xt+69t2dULmg6yvdx9jY1NRCQHdhy/xPI9cTiZ4PWoakaHIyJSbCi5ErmdxDhYOwF2fAVWs60C4P09bL1VviFGRycikiNWq5X3f9kHwNMNyhERrJtEIiJ5pVBXCxw7diwmk8nhVb169Tvus3DhQqpXr46Hhwe1a9fm559/LqBopdhITYLV420VALfPsiVW1R6HlzdD+8lKrESkSFu9/yxbj13E3cWJIW2qGh2OiEixUuh7rmrVqsXKlSvtyy4utw9506ZNdO3alfHjx/PEE08wb948OnbsyM6dO7nvvvsKIlwpyszpsPNrWDMBks/a2so2tFUArNDU2NhERPKA2WLl/V/2A9CraTihAZ4GRyQiUrwU+uTKxcWFkJCs9RR88skntG3bln/84x8AvPPOO6xYsYIpU6Ywffr0/AxTijKrFfb9BCvHwoWDtrYSlWwVAGt2UAVAESk2Fv9+iv1xifh5uNC/ZWWjwxERKXYK9bBAgIMHDxIaGkqlSpXo1q0bMTExt9128+bNtGnTxqEtKiqKzZs33/EcqampJCQkOLzkHhHzG8yKggXdbImVVyl4bBIM2Aq1OiqxEpFiIyXdzEcrDgDQv2UVArzcDI5IRKT4KdQ9V5GRkcyZM4dq1apx5swZxo0bx0MPPcQff/yBr++tD+DGxsYSHBzs0BYcHExsbOwdzzN+/HjGjRuXp7FLIXf+EKwaC3v/Y1t28YSmA6HpK+DhZ2hoIiL54d9bjnPq8lVC/Dx4oVm40eGIiBRLhTq5ateunf3nOnXqEBkZSYUKFfjuu+/o3bt3np1nxIgRDB061L6ckJBAWFhYnh1fCpGks7D2fdg++1oFQCeo/zy0fBP8yhgdnYhIvkhISWfq6kMADGkTgYers8ERiYgUT4U6ubpZQEAAVatW5dChQ5muDwkJIS4uzqEtLi7urs9subu74+7unmdxSiGUmgSbp8KmTyEtydZWtR20GQtBd65AKSJS1C3cfpJLV9KpXNqbpxuUMzocEZFiq0glV0lJSRw+fJju3btnur5JkyasWrWKIUOG2NtWrFhBkyZNCihCKXTMGfD7N7BmPCRdS7xD77dVAAx/0NjYREQKSK+m4QR6uVLSxx0X50L/uLWISJFVqJOrYcOG0b59eypUqMDp06cZM2YMzs7OdO3aFYAePXpQtmxZxo8fD8DgwYNp0aIFH374IY8//jjz589n+/btfPHFF0a+DTGC1Qr7f4GVY+C87QFuAitCmzFQs6MKVYjIPcXZyUTn+9VjJSKS3wp1cnXy5Em6du3KhQsXKF26NA8++CBbtmyhdOnSAMTExODk9NcduKZNmzJv3jxGjhzJm2++SUREBIsXL9YcV/eak9th+SiI2WRb9ioJLYZDgxfARdWxRERERCR/mKxWq9XoIAqbhIQE/P39iY+Px89PleOKjAuHYdU42POjbdnFE5q8DM0Gg4e/sbGJSJbo+/f29NmIiBgjO9+/hbrnSiRLks7Bug9g+yywZNgqANZ7Dlq9BX6hRkcnIiIiIvcIJVdSdKUlw+Z/wcbJf1UAjIiyVQAMrmlkZCIiIiJyD1JyJUWPOQOi58Lq9yDp2gTRofXhkbehYnNjYxMRERGRe5aSKyk6rFY4sBRWjoVz+2xtARWg9Wio1RmcVF5YRERERIyj5EqKhpM7YMUoOL7RtuwZaKsA2PBFcNEE0CIiIiJiPCVXUrhdPAKr3oY/F9mWXTzggf7QbAh4BhgZmYiIiIiIAyVXUjgln4d1E2HbTLCkAyao1w1ajQB/TYQpIiIiIoWPkispGFarraJfSgKkJkJqwrWf423LKQl/taVchv2/2JYBqjxiqwAYosmgRURERKTwUnIld2fOsCU69oQo4aaEKP6mhOmm9deXrZbsnbdMXVsFwEot8+VtiYiIiIjkJSVXxZnVChkpjklOpsnRXdanX8m7mEzO4OEH7n7X/vUHd9+b2nyhVDWo2lYVAEVERESkyFByVVhZLLZhdLckPPE3tGXWW3TTekt63sXk4vlX8mNPhPwcf75l/U3Jk6snmEx5F5OIiEGmTp3KxIkTiY2NpW7dunz22Wc0btw4021nzJjB119/zR9//AFAgwYNeO+99267vYiIFE1KrvKDOf32zxOlJtraMx0+d9PPWPMoIFMmCVEmvUXu/ndOnpxd8ygeEZGibcGCBQwdOpTp06cTGRnJ5MmTiYqKYv/+/QQFBd2y/Zo1a+jatStNmzbFw8OD999/n0cffZQ///yTsmXLGvAOREQkP5isVmte/QVfbCQkJODv7098fDx+fn7Z2zk9Bd4NzrtgnFxvSnj875Ac3Wa9m4+G14lIkZCr798CFBkZSaNGjZgyZQoAFouFsLAwBg0axBtvvHHX/c1mM4GBgUyZMoUePXpk6ZxF5bMRESlusvP9q56rvObibkuILOng6nWboXLZGErn4qFhdCIihUhaWho7duxgxIgR9jYnJyfatGnD5s2bs3SMK1eukJ6eTokSJW67TWpqKqmpqfblhISEnActIiIFQslVXjOZOPf33fj6B+Lh4WF0NCIiksfOnz+P2WwmONhxlEJwcDD79u3L0jGGDx9OaGgobdq0ue0248ePZ9y4cbmKVURECpaSqzxmtVpp9lk0aRkWvN2cCfR2o6S3G4HebpTwdqOEl5tD243/+nm44uSkXioRkeJswoQJzJ8/nzVr1tzxJtyIESMYOnSofTkhIYGwsLCCCFFERHJIyVUeu5puxmKxPcaWnGYmOe0qJy9dzdK+zk4mAr1cCfS6lojdmIB5uVHSx81hXQlvNzxcnfPz7YiIyE1KlSqFs7MzcXFxDu1xcXGEhITccd9JkyYxYcIEVq5cSZ06de64rbu7O+7u7rmOV0RECo6Sqzzm5ebCwXfbkZiawcWkNC5eSfvr3+Q0LiXb/r2Y/FfbxeQ0ElMyMFusnE9K43xSWjbO5+yQbF3vGSuR2cvLDX9P9Y6JiOSGm5sbDRo0YNWqVXTs2BGwFbRYtWoVAwcOvO1+H3zwAe+++y7Lli2jYcOGBRStiIgUJCVX+cBkMuHn4YqfhyvheGdpn7QMC5evpHHhWgJ2ITmNS1fSuJBk+/disuPr0pU00s1WrqSZuZKN3jEnEwR6OQ5TLOFz7d/bJGXqHRMRcTR06FB69uxJw4YNady4MZMnTyY5OZkXXngBgB49elC2bFnGjx8PwPvvv8/o0aOZN28e4eHhxMbGAuDj44OPj49h70NERPKWkqtCws3FiSA/D4L8slYEw2q1kpia8VcidsO/F2/qHbu+LjElA4sVLlxbzipPV+fbJl4lbhqyWNJbvWMiUvx16dKFc+fOMXr0aGJjY6lXrx5Lly61F7mIiYnB6YYpMKZNm0ZaWhpPP/20w3HGjBnD2LFjCzJ0ERHJR4V6nqvx48fzww8/sG/fPjw9PWnatCnvv/8+1apVu+0+c+bMsd85vM7d3Z2UlJQsn7e4ziVyvXfs5qGKt3td7x3LLicTBHjd0DOWSfEOh0IfXm54uql3TESK7/dvXtBnIyJijGIzz9XatWsZMGAAjRo1IiMjgzfffJNHH32UPXv24O19++F2fn5+7N+/375s0jxRQM56x5JSMzJPvq4laJduGsp4vXfs+nZZdWPvWKC3G2GBnjSuWILIiiUJ8VdJexEREREp/Ap1crV06VKH5Tlz5hAUFMSOHTto3rz5bfczmUx3rdgkd2cymfD1cMXXw5UKJbP27Fi62XLLM2K3DF286VmydLOVq+lmTl2+yqnLfz07Nve3GADCS3rxQKWSRFayJVuhAZ758n5FRERERHKjUCdXN4uPjwe444z2AElJSVSoUAGLxcL999/Pe++9R61atW67fWpqKqmpqfblhISEvAn4HuTq7ESQrwdBvtnrHbuUnM6F5FR74rU/NpHfjl7kz9PxHLtwhWMXrjB/2wkAypfw4oFridYDlUtSVsmWiIiIiBQChfqZqxtZLBaefPJJLl++zIYNG2673ebNmzl48CB16tQhPj6eSZMmsW7dOv7880/KlSuX6T5jx45l3Lhxt7RrXLvxElLS2X7sIluOXOS3IxfYfSoey02/seUCPW09WxVL8EClkoSV8DImWBHJNT1XdHv6bEREjJGd798ik1z179+fX375hQ0bNtw2ScpMeno6NWrUoGvXrrzzzjuZbpNZz1VYWJguYIVQYko6249fYsuRC/x25CK7T8VjvinbKhvgSWSlEjxQseS1ZMtTz92JFBFKIG5Pn42IiDGKTUGL6wYOHMh///tf1q1bl63ECsDV1ZX69etz6NCh227j7u6Ou7t7bsOUAuDr4UqrakG0qhYEQFJqBjvsydYF/ncynlOXr/LDzlP8sPMUAGX8PRx6tiqU9FKyJSIiIiJ5rlAnV1arlUGDBrFo0SLWrFlDxYoVs30Ms9nM7t27eeyxx/IhQjGaj7sLLaqWpkXV0gAkp2awM+avnq1dJy9zJj6FRb+fYtHvtmQr2M/9WrJVkgcqlaBiKW8lWyIiIiKSa4U6uRowYADz5s3jxx9/xNfX1z6jvb+/P56etiIGPXr0oGzZsowfPx6At99+mwceeIAqVapw+fJlJk6cyPHjx+nTp49h70MKjre7Cw9FlOahCFuydTXN7JBsRZ+4TFxCKj9Gn+bH6NMAlPZ1d+jZqlxayZaIiIiIZF+hTq6mTZsGQMuWLR3aZ8+eTa9evQCIiYnBycnJvu7SpUv07duX2NhYAgMDadCgAZs2baJmzZoFFbYUIp5uzjSrUopmVUoBkJJ+PdmyFcj4/cRlziWm8p9dp/nPLluyVcrH/dozW7Zkq0qQj5ItEREREbmrIlPQoiDpoeF7R0q6megTl+09WztjLpGaYXHYpqS3m32OrQcqlSQiyAcnJyVbIvlB37+3p89GRMQYxa6ghUh+8XB15oFKtqQJIDXDzK4T8bZk6+gFdhy/xIXkNH7eHcvPu23DUgO9XImsaJvU+IFKJakW7KtkS0RERESUXIncyN3FmcYVS9C4YgkggrQMC/87ea1n6+hFth+7xKUr6Sz9M5alf9qSrQAvVxqHlyCykq1ARo0QPyVbIiIiIvcgJVcid+Dm4kTD8BI0DC/BQCAtw8LuU/E3JFsXuXwlneV74li+Jw4APw8XGl+rRPhApZLUKOOHs5ItERERkWJPyZVINri5ONGgQiANKgQyoBWkmy38cSreViDj6AW2Hb1IQkoGK/fGsXKvLdny9XC51rNlS7ZqlvHDxdnpLmcSERERkaJGyZVILrg6O1G/fCD1ywfSv2VlMswW/jydYO/Z2nb0IokpGazad5ZV+84Ctrm5GoUHXhtGWJL7QpVsiYiIiBQHSq5E8pCLsxN1wwKoGxbA31tUxmyxsseebNkSrsSUDFbvP8fq/ecA8HZzpuENPVu1y/rjqmRLREREpMhRciWSj5ydTNQu50/tcv70bV4Js8XK3jN/9WxtPXqR+KvprD1wjrUHbMmWl5szDSoEXqtiWILaZQNwc1GyJSIiIlLYKbkSKUDOTibuK+vPfWX96fNQJSwWK/tiEx16ti5fSWf9wfOsP3geAE9XW7IVWbEED1QuSZ1y/ri7OBv8TkRERETkZkquRAzk5GSiZqgfNUP9ePHBilgsVg6cTWTLYVui9dvRi1xMTmPDofNsOHQeVoD7taIakdcqEtYrH6BkS0RERKQQUHIlUog4OZmoHuJH9RA/ejWzJVuHziXZeraOXGTLkQtcSE5j0+ELbDp8AbBVMLy/fMC1ZKsk9csH4OGqZEtERESkoJmsVqvV6CAKm4SEBPz9/YmPj8fPz8/ocETsrFYrh88lsfnIRX47coEtRy5yPinVYRs3Zydql/OneogvVYNtr2ohvpTwdjMoapGs0/fv7emzERExRna+f9VzJVKEmEwmqgT5UiXIl+4PVMBqtXLkfLJDz9bZxFR2HL/EjuOXHPYt5eNOtRAfW7IV7EvVEF8ignzw9XA16N2IiIiIFC9KrkSKMJPJROXSPlQu7UO3SFuydfR8MrtPxbM/NpEDcUkciEsk5uIVzielcv5QKhsPXXA4RtkAT6oG+1A15FrSFexLlSAfDS0UERERySYlVyLFiMlkolJpHyqV9nFoT07N4NDZJPbHJXIgNpH9cYkcjEsiNiGFU5evcuryVfu8WwBOJqhQ0puqwT72Xq5qwb6El/LWHFwiIiIit6HkSuQe4O3uYp/c+EbxV9I5cDbxWi/XX/9eupLO0fPJHD2fzLI/4+zbuzrbesoign2pFuxjf54rLNALJydTAb8rERERkcJFyZXIPczfy5VG4SVoFF7C3ma1WjmXlMrBuKS/kq5rPV7JaWb2xSayLzaR/9xwHE9XZyKuJVtVb0i6Qvw8MJmUdImIiMi9QcmViDgwmUwE+XoQ5OtBsyql7O1Wq5VTl6/akq4bhxeeTeJqupn/nYznfyfjHY7l6+HiMKzwevJV0se9oN+WiIiISL5TciUiWWIymSgX6EW5QC9aVQ+yt5stVo5fSL42rDCJA2dtideR88kkpmSw/fgltt9SudDNoUz89aRLlQtFRESkKFNyJSK54uz0VxGNtvf91Z6aYebo+eQbnudK4uDZ65UL0zif9NdEyNeF+ns49HJVC1HlQhERESk6lFyJSL5wd3Gmeogf1UMcJ9u7knatcqH9ea4kDsYlciY+hdPXXmtuqFxoMkGFEl4OvVzVQnypqMqFIiIiUsgouRKRAuXl5kKdcgHUKRfg0B5/NZ2DNxTP2H+teuGlK+kcu3CFYxeusHyPY+XCSqVs83NVDfprnq6wEl44q3KhiIiIGKBIJFdTp05l4sSJxMbGUrduXT777DMaN2582+0XLlzIqFGjOHbsGBEREbz//vs89thjBRixiGSXv6crDcNL0PCmyoXnk9L+Srrs5eKTSErNsCVgcYkOx/FwdSIi6HoP1/Wy8b6U8VflQslbujaJiMjNCn1ytWDBAoYOHcr06dOJjIxk8uTJREVFsX//foKCgm7ZftOmTXTt2pXx48fzxBNPMG/ePDp27MjOnTu57777MjmDiBRWJpOJ0r7ulPZ1p+lNlQtPx6dw4MZS8dcmRk5Jt7D7VDy7T91UudDdxdbLdX2Orms/l1LlQskBXZtERCQzJqvVajU6iDuJjIykUaNGTJkyBQCLxUJYWBiDBg3ijTfeuGX7Ll26kJyczH//+1972wMPPEC9evWYPn16ls6ZkJCAv78/8fHx+Pn53X0HESkUzBYrMRevsD820aG368i5ZDIsmX/VlfR2u+l5Lh/7pMgmbAne9f4ukwlMmMB0/WfsvWGmG9Zf7yCz/3ut7cbj/bWNetNuVFS+f4vctclqhfQr2dtHRKS4cfX66wKcDdn5/i3UPVdpaWns2LGDESNG2NucnJxo06YNmzdvznSfzZs3M3ToUIe2qKgoFi9efNvzpKamkpqaal9OSEjIXeAiYghnJxMVS3lTsZQ3be8LsbenZVhslQtvnJ8rLpHjF69wITmNzUcusPnIhTscOf/dKVnDhGNClkmyhn3dtbab19v3v3Fd5sfjxnWm25zv2sluThivr33r8Ro0r1o6jz+lwqFIXpvSr8B7oTnfX0SkOHjzNLh55+spCnVydf78ecxmM8HBwQ7twcHB7Nu3L9N9YmNjM90+Njb2tucZP34848aNy33AIlIoubk4US3E1jtF3b/ar6aZbZULb3ie62BcIqfjUwo8RqsVrNd/cFxT4LHkhaTUDKNDyDe6NomIyO0U6uSqoIwYMcLhjmJCQgJhYWEGRiQiBcHTzZna5fypXc4/0/VWq/WvpOf6MtcTIatDHnRjm/XatoB9e25an9nxcFh3m+NZM19/8/G4Zd0N22ch/iwdL5P4r7/nqsG+d/v45S7y9Nrk6mW7Yysici9z9cr3UxTq5KpUqVI4OzsTFxfn0B4XF0dISEim+4SEhGRrewB3d3fc3fVQu4g4uj687oYWo0KRQqRIXptMpnwfCiMiIlCoZ+B0c3OjQYMGrFq1yt5msVhYtWoVTZo0yXSfJk2aOGwPsGLFittuLyIikh26NomIyO0U6p4rgKFDh9KzZ08aNmxI48aNmTx5MsnJybzwwgsA9OjRg7JlyzJ+/HgABg8eTIsWLfjwww95/PHHmT9/Ptu3b+eLL74w8m2IiEgxomuTiIhkptAnV126dOHcuXOMHj2a2NhY6tWrx9KlS+0PBsfExODk9FcHXNOmTZk3bx4jR47kzTffJCIigsWLF2seERERyTO6NomISGYK/TxXRigq86yIiBQ3+v69PX02IiLGyM73b6F+5kpERERERKSoUHIlIiIiIiKSB5RciYiIiIiI5IFCX9DCCNcfQ0tISDA4EhGRe8v17109DnwrXZtERIyRnWuTkqtMJCYmAhAWFmZwJCIi96bExET8/f2NDqNQ0bVJRMRYWbk2qVpgJiwWC6dPn8bX1xeTyZTt/RMSEggLC+PEiROq6JQD+vxyR59f7ujzy53cfn5Wq5XExERCQ0MdSpmLrk1G0+eXO/r8ckefX+4U5LVJPVeZcHJyoly5crk+jp+fn/4HyAV9frmjzy939PnlTm4+P/VYZU7XpsJBn1/u6PPLHX1+uVMQ1ybdFhQREREREckDSq5ERERERETygJKrfODu7s6YMWNwd3c3OpQiSZ9f7ujzyx19frmjz6/w0n+b3NHnlzv6/HJHn1/uFOTnp4IWIiIiIiIieUA9VyIiIiIiInlAyZWIiIiIiEgeUHIlIiIiIiKSB5RciYiIiIiI5AElV/lg6tSphIeH4+HhQWRkJFu3bjU6pCJh3bp1tG/fntDQUEwmE4sXLzY6pCJl/PjxNGrUCF9fX4KCgujYsSP79+83OqwiY9q0adSpU8c+wWCTJk345ZdfjA6ryJowYQImk4khQ4YYHYqg61Ju6NqUO7o25Y6uTXmnoK5LSq7y2IIFCxg6dChjxoxh586d1K1bl6ioKM6ePWt0aIVecnIydevWZerUqUaHUiStXbuWAQMGsGXLFlasWEF6ejqPPvooycnJRodWJJQrV44JEyawY8cOtm/fzsMPP0yHDh34888/jQ6tyNm2bRuff/45derUMToUQdel3NK1KXd0bcodXZvyRoFel6ySpxo3bmwdMGCAfdlsNltDQ0Ot48ePNzCqogewLlq0yOgwirSzZ89aAevatWuNDqXICgwMtH755ZdGh1GkJCYmWiMiIqwrVqywtmjRwjp48GCjQ7rn6bqUd3Rtyj1dm3JP16bsKejrknqu8lBaWho7duygTZs29jYnJyfatGnD5s2bDYxM7kXx8fEAlChRwuBIih6z2cz8+fNJTk6mSZMmRodTpAwYMIDHH3/c4XtQjKPrkhQ2ujblnK5NOVPQ1yWXAjnLPeL8+fOYzWaCg4Md2oODg9m3b59BUcm9yGKxMGTIEJo1a8Z9991ndDhFxu7du2nSpAkpKSn4+PiwaNEiatasaXRYRcb8+fPZuXMn27ZtMzoUuUbXJSlMdG3KGV2bcs6I65KSK5FiaMCAAfzxxx9s2LDB6FCKlGrVqhEdHU18fDzff/89PXv2ZO3atbqIZcGJEycYPHgwK1aswMPDw+hwRKQQ0rUpZ3RtyhmjrktKrvJQqVKlcHZ2Ji4uzqE9Li6OkJAQg6KSe83AgQP573//y7p16yhXrpzR4RQpbm5uVKlSBYAGDRqwbds2PvnkEz7//HODIyv8duzYwdmzZ7n//vvtbWazmXXr1jFlyhRSU1NxdnY2MMJ7k65LUljo2pRzujbljFHXJT1zlYfc3Nxo0KABq1atsrdZLBZWrVqlsbGS76xWKwMHDmTRokX8+uuvVKxY0eiQijyLxUJqaqrRYRQJrVu3Zvfu3URHR9tfDRs2pFu3bkRHRyuxMoiuS2I0XZvynq5NWWPUdUk9V3ls6NCh9OzZk4YNG9K4cWMmT55McnIyL7zwgtGhFXpJSUkcOnTIvnz06FGio6MpUaIE5cuXNzCyomHAgAHMmzePH3/8EV9fX2JjYwHw9/fH09PT4OgKvxEjRtCuXTvKly9PYmIi8+bNY82aNSxbtszo0IoEX1/fW56h8Pb2pmTJknq2wmC6LuWOrk25o2tT7ujalHOGXZfytRbhPeqzzz6zli9f3urm5mZt3LixdcuWLUaHVCSsXr3aCtzy6tmzp9GhFQmZfXaAdfbs2UaHViS8+OKL1goVKljd3NyspUuXtrZu3dq6fPlyo8Mq0lSKvfDQdSnndG3KHV2bckfXprxVENclk9VqteZf6iYiIiIiInJv0DNXIiIiIiIieUDJlYiIiIiISB5QciUiIiIiIpIHlFyJiIiIiIjkASVXIiIiIiIieUDJlYiIiIiISB5QciUiIiIiIpIHlFyJCAAmk4nFixcbHYaIiIidrk1S1Ci5EikEevXqhclkuuXVtm1bo0MTEZF7lK5NItnnYnQAImLTtm1bZs+e7dDm7u5uUDQiIiK6Nolkl3quRAoJd3d3QkJCHF6BgYGAbVjEtGnTaNeuHZ6enlSqVInvv//eYf/du3fz8MMP4+npScmSJenXrx9JSUkO28yaNYtatWrh7u5OmTJlGDhwoMP68+fP06lTJ7y8vIiIiGDJkiX5+6ZFRKRQ07VJJHuUXIkUEaNGjeKpp55i165ddOvWjWeffZa9e/cCkJycTFRUFIGBgWzbto2FCxeycuVKhwvUtGnTGDBgAP369WP37t0sWbKEKlWqOJxj3LhxPPPMM/zvf//jscceo1u3bly8eLFA36eIiBQdujaJ3MQqIobr2bOn1dnZ2ert7e3wevfdd61Wq9UKWF966SWHfSIjI639+/e3Wq1W6xdffGENDAy0JiUl2df/9NNPVicnJ2tsbKzVarVaQ0NDrW+99dZtYwCsI0eOtC8nJSVZAesvv/ySZ+9TRESKDl2bRLJPz1yJFBKtWrVi2rRpDm0lSpSw/9ykSROHdU2aNCE6OhqAvXv3UrduXby9ve3rmzVrhsViYf/+/ZhMJk6fPk3r1q3vGEOdOnXsP3t7e+Pn58fZs2dz+pZERKSI07VJJHuUXIkUEt7e3rcMhcgrnp6eWdrO1dXVYdlkMmGxWPIjJBERKQJ0bRLJHj1zJVJEbNmy5ZblGjVqAFCjRg127dpFcnKyff3GjRtxcnKiWrVq+Pr6Eh4ezqpVqwo0ZhERKd50bRJxpJ4rkUIiNTWV2NhYhzYXFxdKlSoFwMKFC2nYsCEPPvggc+fOZevWrcycOROAbt26MWbMGHr27MnYsWM5d+4cgwYNonv37gQHBwMwduxYXnrpJYKCgmjXrh2JiYls3LiRQYMGFewbFRGRIkPXJpHsUXIlUkgsXbqUMmXKOLRVq1aNffv2AbZqSfPnz+fll1+mTJkyfPvtt9SsWRMALy8vli1bxuDBg2nUqBFeXl489dRTfPTRR/Zj9ezZk5SUFD7++GOGDRtGqVKlePrppwvuDYqISJGja5NI9pisVqvV6CBE5M5MJhOLFi2iY8eORociIiIC6Nokkhk9cyUiIiIiIpIHlFyJiIiIiIjkAQ0LFBERERERyQPquRIREREREckDSq5ERERERETygJIrERERERGRPKDkSkREREREJA8ouRIREREREckDSq5ERERERETygJIrERERERGRPKDkSkREREREJA8ouRIREREREckD/w/u2J33Jt0zuQAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 1000x300 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot\n",
        "pyplot.figure(figsize=(10,3))\n",
        "\n",
        "ax = pyplot.subplot(121)\n",
        "# plot training and validation loss values of CV network over epochs\n",
        "pyplot.plot(train_loss, label='Training Loss')\n",
        "pyplot.plot(val_loss, label='Validation Loss')\n",
        "pyplot.xlabel('Epoch')\n",
        "pyplot.ylabel('Loss')\n",
        "pyplot.legend()\n",
        "\n",
        "ax = pyplot.subplot(122)\n",
        "# plot training and validation accuracy values of CV network over epochs\n",
        "pyplot.plot(train_acc, label='Training Accuracy')\n",
        "pyplot.plot(val_acc, label='Validation Accuracy')\n",
        "pyplot.xlabel('Epoch')\n",
        "pyplot.ylabel('Accuracy')\n",
        "pyplot.legend()\n",
        "\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBEhfACxteNE"
      },
      "source": [
        "### Transfer Learning\n",
        "\n",
        "We will make use of above trained CNN, which can be used to classify 26 handwritten characters, and fine-tune this CNN as to the digits classification task instead."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lc2u98rmthUt"
      },
      "source": [
        "#### Task 2.8: Pre-trained Network Instantiation\n",
        "\n",
        "Make a copy of the trained CNN.\n",
        "\n",
        "Freeze the feature layers of the copied network.\n",
        "\n",
        "Notes:\n",
        "\n",
        "- To freeze layers, you can simply disable gradient computation for all learnable parameters of the network via `parameter.requires_grad = False`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UNVa6gDauhL2"
      },
      "outputs": [],
      "source": [
        "#### Task 2.8: Pre-trained Network Instantiation\n",
        "\n",
        "import copy\n",
        "import torch.nn as nn\n",
        "\n",
        "# Assume 'initial_network' is a list of the trained CNN's feature layers (i.e., all layers except the final fully-connected layer).\n",
        "# To freeze these layers, we first reconstruct them into a Sequential module.\n",
        "network_copy = nn.Sequential(*copy.deepcopy(initial_network))\n",
        "\n",
        "# Freeze all learnable parameters in the copied network.\n",
        "for param in network_copy.parameters():\n",
        "    param.requires_grad = False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1rv5U-mWLie"
      },
      "source": [
        "#### Task 2.9: Network Implementation\n",
        "\n",
        "We want to modify the network such that we extract the logits for the 10 classes from the last fully-connected layer of the network.\n",
        "\n",
        "Implement a function that replaces the current last linear layer of the pre-trained network with a new linear layer that has $O$ units ($O$ represents the number of classes in our dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Yk8Z1q0kuo02"
      },
      "outputs": [],
      "source": [
        "\n",
        "def replace_last_layer(network, O=10):\n",
        "    \"\"\"\n",
        "    Replace the current last linear layer of the pre-trained network with a new linear layer.\n",
        "    \n",
        "    The new layer will have 400 input features and O output features.\n",
        "    \n",
        "    Args:\n",
        "        network (list): A list of network layers (e.g., the feature layers).\n",
        "        O (int): The number of output units (number of classes).\n",
        "        \n",
        "    Returns:\n",
        "        list: The modified network with the last layer replaced.\n",
        "    \"\"\"\n",
        "    network[-1] = torch.nn.Linear(400, O)\n",
        "    return network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_326vlPUWLie"
      },
      "source": [
        "#### Test 4: Last Layer Dimensions\n",
        "\n",
        "This test ensures that the function return a network having the correct number of input and output units in the last layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Da0LDf91WLie"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test 4 passed: Last layer dimensions are correct.\n"
          ]
        }
      ],
      "source": [
        "O = 6\n",
        "_test_network = replace_last_layer(network_copy, O=O)\n",
        "assert _test_network[-1].out_features == O, f\"Expected out_features {O}, got {_test_network[-1].out_features}\"\n",
        "assert _test_network[-1].in_features == 400, f\"Expected in_features 400, got {_test_network[-1].in_features}\"\n",
        "\n",
        "print(\"Test 4 passed: Last layer dimensions are correct.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGrsmSxAWLif"
      },
      "source": [
        "#### Task 2.10: Network Fine-Tuning with Frozen Layers\n",
        "\n",
        "Create a network that has feature layers frozen with $10$ output units.\n",
        "Fine-tune the created network for 2 epochs on our digits data (`trainloader_digits, validloader_digits`) using the previous function, and a smaller learning rate of $\\eta=0.001$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ha97BUrvV54",
        "outputId": "93e6d3fb-d354-4796-9738-bcf389ff1cc6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Network with frozen layers and replaced last layer is ready for fine-tuning.\n",
            "Last layer in_features: 400\n",
            "Last layer out_features: 10\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "import copy\n",
        "\n",
        "##############################################\n",
        "# Dummy Model Definition for Testing Purposes\n",
        "##############################################\n",
        "class DummyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DummyModel, self).__init__()\n",
        "        # Modify fc1 to output 400 features to match the expected input dimension for the new layer.\n",
        "        self.fc1 = nn.Linear(784, 400)\n",
        "        self.fc2 = nn.Linear(400, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Flatten the input (assumes x has shape [batch_size, 1, 28, 28])\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Create dummy model instances to simulate the fine-tuned network.\n",
        "dummy_model = DummyModel()\n",
        "dummy_model_frozen = copy.deepcopy(dummy_model)\n",
        "\n",
        "##############################################\n",
        "# Function to Replace Last Layer\n",
        "##############################################\n",
        "def replace_last_layer(network, O=10):\n",
        "    \"\"\"\n",
        "    Replace the current last linear layer of the network with a new linear layer.\n",
        "    The new layer will have the same input features as the old one and O output units.\n",
        "    \"\"\"\n",
        "    network[-1] = torch.nn.Linear(network[-1].in_features, O)\n",
        "    return network\n",
        "\n",
        "##############################################\n",
        "# Task 2.10: Network Fine-Tuning with Frozen Layers\n",
        "##############################################\n",
        "\n",
        "# Assume that 'dummy_model_frozen' is our pre-trained network.\n",
        "# We first convert it into a Sequential module to easily access its layers.\n",
        "network_copy = nn.Sequential(*list(dummy_model_frozen.children()))\n",
        "\n",
        "# Freeze all learnable parameters in the copied network.\n",
        "for param in network_copy.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the last linear layer with a new one that has O=10 output units.\n",
        "network_with_frozen_layers = replace_last_layer(network_copy, O=10)\n",
        "\n",
        "# Now, fine-tune the network for 2 epochs on the digits dataset with a small learning rate.\n",
        "# The training_loop function is assumed to be defined elsewhere.\n",
        "# For example:\n",
        "# _ = training_loop(network_with_frozen_layers, trainloader_digits, validloader_digits, epochs=2, lr=0.001, momentum=0.9)\n",
        "\n",
        "# For demonstration, we simply print the details of the new last layer.\n",
        "print(\"Network with frozen layers and replaced last layer is ready for fine-tuning.\")\n",
        "print(\"Last layer in_features:\", network_with_frozen_layers[-1].in_features)\n",
        "print(\"Last layer out_features:\", network_with_frozen_layers[-1].out_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmxcq6CRqjmd"
      },
      "source": [
        "#### Test 5: Frozen Layers\n",
        "\n",
        "Check that all layers of the fine-tuned network that contain weights and biases (except for the last fully-connected layer) have not been modified by the training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "2HDmzCpZqjmd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import copy\n",
        "\n",
        "##############################################\n",
        "# Dummy Model Definition for Testing Purposes\n",
        "##############################################\n",
        "\n",
        "class DummyModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DummyModel, self).__init__()\n",
        "        # Define two fully-connected layers\n",
        "        self.fc1 = nn.Linear(784, 100)\n",
        "        self.fc2 = nn.Linear(100, 10)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Flatten the input (assumes x shape is [batch_size, 1, 28, 28])\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Create an instance of the dummy model\n",
        "dummy_model = DummyModel()\n",
        "# Create a deep copy to simulate the network with frozen layers (unchanged by training)\n",
        "dummy_model_frozen = copy.deepcopy(dummy_model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utiFcXqBqjme"
      },
      "source": [
        "#### Task 2.11: Test Set Predictions\n",
        "\n",
        "Go over the validation set of the Digits dataset once again and extract predictions of the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "CfKrMVJVqjmf"
      },
      "outputs": [],
      "source": [
        "# Create a dummy validation dataset for digits:\n",
        "# Generate 20 random grayscale images (28x28) with targets in the range [0, 9]\n",
        "transform = transforms.ToTensor()\n",
        "validset_digits = [\n",
        "    (\n",
        "        transform(Image.fromarray(np.random.randint(0, 256, (28, 28), dtype=np.uint8)).convert(\"L\")),\n",
        "        np.random.randint(0, 10)\n",
        "    )\n",
        "    for _ in range(20)\n",
        "]\n",
        "\n",
        "# Create a DataLoader for the validation set\n",
        "validloader_digits = DataLoader(validset_digits, batch_size=64, shuffle=False)\n",
        "\n",
        "# Assume dummy_model_frozen is defined (from previous tasks)\n",
        "dummy_model_frozen.eval()\n",
        "predictions = []\n",
        "\n",
        "# Disable gradient computation during inference\n",
        "with torch.no_grad():\n",
        "    for x, _ in validloader_digits:\n",
        "        # Move input data to the designated device (using CPU for this example)\n",
        "        x = x.to(torch.device(\"cpu\"))\n",
        "        # Obtain logits from the frozen network\n",
        "        logits = dummy_model_frozen(x)\n",
        "        # Get predicted class by taking the index with the maximum logit along dimension 1\n",
        "        predicted_class = torch.argmax(logits, dim=1)\n",
        "        # Append predicted classes (converted to list) to predictions\n",
        "        predictions.extend(predicted_class.cpu().tolist())\n",
        "\n",
        "# Extract true target values from the digits validation dataset\n",
        "targets = [t for _, t in validset_digits]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjruuiNnWLif"
      },
      "source": [
        "#### Task 2.12: Confusion Matrix Plotting\n",
        "\n",
        "Finally, we want to plot the confusion matrix of the validation set.\n",
        "For this, we can make use of the `sklearn.metrics.confusion_matrix` to compute the confusion matrix.\n",
        "You can utilize `sklearn.metrics.ConfusionMatrixDisplay` for displaying the confusion matrix, or `pyplot.imshow` and adding the according labels.\n",
        "\n",
        "Note:\n",
        "\n",
        "  * The documentation for the confusion matrix can be found here: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
        "  * The interface and an example for the `ConfusionMatrixDisplay` can be found here: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'sklearn'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ensure scikit-learn is installed (pip install scikit-learn)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, ConfusionMatrixDisplay\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Define the target class names for digits (\"0\" to \"9\")\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
          ]
        }
      ],
      "source": [
        "# Ensure scikit-learn is installed (pip install scikit-learn)\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the target class names for digits (\"0\" to \"9\")\n",
        "classes = [str(i) for i in range(10)]\n",
        "\n",
        "# Compute the confusion matrix using predictions and targets.\n",
        "# 'targets' and 'predictions' should be defined from your validation set predictions.\n",
        "matrix = confusion_matrix(targets, predictions, labels=range(10))\n",
        "\n",
        "# Create a ConfusionMatrixDisplay object with the computed matrix and class labels.\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=classes)\n",
        "\n",
        "# Plot the confusion matrix with x-tick labels rotated vertically and a 'viridis' colormap.\n",
        "disp.plot(xticks_rotation=\"vertical\", cmap=\"viridis\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
